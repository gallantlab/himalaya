
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_auto_examples/multiple_kernel_ridge/plot_mkr_sklearn_api.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download__auto_examples_multiple_kernel_ridge_plot_mkr_sklearn_api.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__auto_examples_multiple_kernel_ridge_plot_mkr_sklearn_api.py:


Multiple kernel ridge with scikit-learn API
===========================================
This example demonstrates how to solve multiple kernel ridge regression, using
scikit-learn API.

.. GENERATED FROM PYTHON SOURCE LINES 7-19

.. code-block:: default


    import numpy as np
    import matplotlib.pyplot as plt

    from himalaya.backend import set_backend
    from himalaya.kernel_ridge import KernelRidgeCV
    from himalaya.kernel_ridge import MultipleKernelRidgeCV
    from himalaya.kernel_ridge import Kernelizer
    from himalaya.kernel_ridge import ColumnKernelizer

    from sklearn.pipeline import make_pipeline








.. GENERATED FROM PYTHON SOURCE LINES 21-28

In this example, we use the ``torch`` backend.

Torch can perform computations both on CPU and GPU.
To use the CPU, use the "torch" backend.
To use GPU, you can either use the "torch" backend and move your data to GPU
with the ``.cuda`` method, or you can use the "torch_cuda" backend which calls
this method in ``backend.asarray``.

.. GENERATED FROM PYTHON SOURCE LINES 28-31

.. code-block:: default


    backend = set_backend("torch_cuda")








.. GENERATED FROM PYTHON SOURCE LINES 32-38

Generate a random dataset
-------------------------
- Xs_train : list of arrays of shape (n_samples_train, n_features)
- Xs_test : list of arrays of shape (n_samples_test, n_features)
- Y_train : array of shape (n_samples_train, n_targets)
- Y_test : array of shape (n_repeat, n_samples_test, n_targets)

.. GENERATED FROM PYTHON SOURCE LINES 38-59

.. code-block:: default


    n_samples_train = 1000
    n_samples_test = 300
    n_targets = 1000
    n_features_list = [1000, 1000, 500]
    feature_names = ["feature space A", "feature space B", "feature space C"]

    Xs_train = [
        backend.randn(n_samples_train, n_features)
        for n_features in n_features_list
    ]
    Xs_test = [
        backend.randn(n_samples_test, n_features) for n_features in n_features_list
    ]
    ws = [
        backend.randn(n_features, n_targets) / n_features
        for n_features in n_features_list
    ]
    Y_train = backend.stack([X @ w for X, w in zip(Xs_train, ws)]).sum(0)
    Y_test = backend.stack([X @ w for X, w in zip(Xs_test, ws)]).sum(0)








.. GENERATED FROM PYTHON SOURCE LINES 60-61

Optional: Add some arbitrary scalings per kernel

.. GENERATED FROM PYTHON SOURCE LINES 61-66

.. code-block:: default

    if True:
        scalings = [0.2, 5, 1]
        Xs_train = [X * scaling for X, scaling in zip(Xs_train, scalings)]
        Xs_test = [X * scaling for X, scaling in zip(Xs_test, scalings)]








.. GENERATED FROM PYTHON SOURCE LINES 67-68

Concatenate the feature spaces and move to GPU with ``backend.asarray``.

.. GENERATED FROM PYTHON SOURCE LINES 68-71

.. code-block:: default

    X_train = backend.asarray(backend.concatenate(Xs_train, 1), dtype="float32")
    X_test = backend.asarray(backend.concatenate(Xs_test, 1), dtype="float32")








.. GENERATED FROM PYTHON SOURCE LINES 72-75

We could precompute the kernels by hand on ``Xs_train``, as done in
``plot_mkr_random_search.py``. Instead, here we use the
``ColumnKernelizer`` to make a ``scikit-learn`` ``Pipeline``.

.. GENERATED FROM PYTHON SOURCE LINES 75-83

.. code-block:: default


    # Find the start and end of each feature space X in Xs
    start_and_end = np.concatenate([[0], np.cumsum(n_features_list)])
    slices = [
        slice(start, end)
        for start, end in zip(start_and_end[:-1], start_and_end[1:])
    ]








.. GENERATED FROM PYTHON SOURCE LINES 84-88

Create a different ``Kernelizer`` for each feature space. Here we use a
linear kernel for all feature spaces, but ``ColumnKernelizer`` accepts any
``Kernelizer``, or ``scikit-learn`` ``Pipeline`` ending with a
``Kernelizer``.

.. GENERATED FROM PYTHON SOURCE LINES 88-95

.. code-block:: default

    kernelizers = [(name, Kernelizer(), slice_)
                   for name, slice_ in zip(feature_names, slices)]
    column_kernelizer = ColumnKernelizer(kernelizers)

    # Note that ``ColumnKernelizer`` has a parameter ``n_jobs`` to parallelize each
    # kernelizer, yet such parallelism does not work with GPU arrays.








.. GENERATED FROM PYTHON SOURCE LINES 96-102

Define the model
----------------

The class takes a number of common parameters during initialization, such as
`kernels` or `solver`. Since the solver parameters might be different
depending on the solver, they can be passed in the `solver_params` parameter.

.. GENERATED FROM PYTHON SOURCE LINES 104-106

Here we use the "random_search" solver.
We can check its specific parameters in the function docstring:

.. GENERATED FROM PYTHON SOURCE LINES 106-110

.. code-block:: default

    solver_function = MultipleKernelRidgeCV.ALL_SOLVERS["random_search"]
    print("Docstring of the function %s:" % solver_function.__name__)
    print(solver_function.__doc__)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Docstring of the function solve_multiple_kernel_ridge_random_search:
    Solve multiple kernel ridge regression using random search.

        Parameters
        ----------
        Ks : array of shape (n_kernels, n_samples, n_samples)
            Input kernels.
        Y : array of shape (n_samples, n_targets)
            Target data.
        n_iter : int, or array of shape (n_iter, n_kernels)
            Number of kernel weights combination to search.
            If an array is given, the solver uses it as the list of kernel weights
            to try, instead of sampling from a Dirichlet distribution.
        concentration : float, or list of float
            Concentration parameters of the Dirichlet distribution.
            If a list, iteratively cycle through the list.
            Not used if n_iter is an array.
        alphas : float or array of shape (n_alphas, )
            Range of ridge regularization parameter.
        score_func : callable
            Function used to compute the score of predictions versus Y.
        cv : int or scikit-learn splitter
            Cross-validation splitter. If an int, KFold is used.
        fit_intercept : boolean
            Whether to fit an intercept. If False, Ks should be centered
            (see KernelCenterer), and Y must be zero-mean over samples.
            Only available if return_weights == 'dual'.
        return_weights : None, 'primal', or 'dual'
            Whether to refit on the entire dataset and return the weights.
        Xs : array of shape (n_kernels, n_samples, n_features) or None
            Necessary if return_weights == 'primal'.
        local_alpha : bool
            If True, alphas are selected per target, else shared over all targets.
        jitter_alphas : bool
            If True, alphas range is slightly jittered for each gamma.
        random_state : int, or None
            Random generator seed. Use an int for deterministic search.
        n_targets_batch : int or None
            Size of the batch for over targets during cross-validation.
            Used for memory reasons. If None, uses all n_targets at once.
        n_targets_batch_refit : int or None
            Size of the batch for over targets during refit.
            Used for memory reasons. If None, uses all n_targets at once.
        n_alphas_batch : int or None
            Size of the batch for over alphas. Used for memory reasons.
            If None, uses all n_alphas at once.
        progress_bar : bool
            If True, display a progress bar over gammas.
        Ks_in_cpu : bool
            If True, keep Ks in CPU memory to limit GPU memory (slower).
            This feature is not available through the scikit-learn API.
        conservative : bool
            If True, when selecting the hyperparameter alpha, take the largest one
            that is less than one standard deviation away from the best.
            If False, take the best.
        Y_in_cpu : bool
            If True, keep the target values ``Y`` in CPU memory (slower).
        diagonalize_method : str in {"eigh", "svd"}
            Method used to diagonalize the kernel.
        return_alphas : bool
            If True, return the best alpha value for each target.

        Returns
        -------
        deltas : array of shape (n_kernels, n_targets)
            Best log kernel weights for each target.
        refit_weights : array or None
            Refit regression weights on the entire dataset, using selected best
            hyperparameters. Refit weights are always stored on CPU memory.
            If return_weights == 'primal', shape is (n_features, n_targets),
            if return_weights == 'dual', shape is (n_samples, n_targets),
            else, None.
        cv_scores : array of shape (n_iter, n_targets)
            Cross-validation scores per iteration, averaged over splits, for the
            best alpha. Cross-validation scores will always be on CPU memory.
        best_alphas : array of shape (n_targets, )
            Best alpha value per target. Only returned if return_alphas is True.
        intercept : array of shape (n_targets,)
            Intercept. Only returned when fit_intercept is True.
    




.. GENERATED FROM PYTHON SOURCE LINES 111-114

We use 100 iterations to have a reasonably fast example (~40 sec).
To have a better convergence, we probably need more iterations.
Note that there is currently no stopping criterion in this method.

.. GENERATED FROM PYTHON SOURCE LINES 114-116

.. code-block:: default

    n_iter = 100








.. GENERATED FROM PYTHON SOURCE LINES 117-118

Grid of regularization parameters.

.. GENERATED FROM PYTHON SOURCE LINES 118-120

.. code-block:: default

    alphas = np.logspace(-10, 10, 41)








.. GENERATED FROM PYTHON SOURCE LINES 121-124

Batch parameters are used to reduce the necessary GPU memory. A larger value
will be a bit faster, but the solver might crash if it runs out of memory.
Optimal values depend on the size of your dataset.

.. GENERATED FROM PYTHON SOURCE LINES 124-137

.. code-block:: default

    n_targets_batch = 1000
    n_alphas_batch = 20
    n_targets_batch_refit = 200

    solver_params = dict(n_iter=n_iter, alphas=alphas,
                         n_targets_batch=n_targets_batch,
                         n_alphas_batch=n_alphas_batch,
                         n_targets_batch_refit=n_targets_batch_refit,
                         jitter_alphas=True)

    model = MultipleKernelRidgeCV(kernels="precomputed", solver="random_search",
                                  solver_params=solver_params)








.. GENERATED FROM PYTHON SOURCE LINES 138-139

Define and fit the pipeline

.. GENERATED FROM PYTHON SOURCE LINES 139-142

.. code-block:: default

    pipe = make_pipeline(column_kernelizer, model)
    pipe.fit(X_train, Y_train)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [                                        ] 0% | 0.00 sec | 100 random sampling with cv |     [                                        ] 1% | 0.51 sec | 100 random sampling with cv |     [                                        ] 2% | 0.96 sec | 100 random sampling with cv |     [.                                       ] 3% | 1.31 sec | 100 random sampling with cv |     [.                                       ] 4% | 1.76 sec | 100 random sampling with cv |     [..                                      ] 5% | 2.21 sec | 100 random sampling with cv |     [..                                      ] 6% | 2.57 sec | 100 random sampling with cv |     [..                                      ] 7% | 2.92 sec | 100 random sampling with cv |     [...                                     ] 8% | 3.39 sec | 100 random sampling with cv |     [...                                     ] 9% | 3.74 sec | 100 random sampling with cv |     [....                                    ] 10% | 4.10 sec | 100 random sampling with cv |     [....                                    ] 11% | 4.45 sec | 100 random sampling with cv |     [....                                    ] 12% | 4.81 sec | 100 random sampling with cv |     [.....                                   ] 13% | 5.16 sec | 100 random sampling with cv |     [.....                                   ] 14% | 5.52 sec | 100 random sampling with cv |     [......                                  ] 15% | 5.87 sec | 100 random sampling with cv |     [......                                  ] 16% | 6.23 sec | 100 random sampling with cv |     [......                                  ] 17% | 6.58 sec | 100 random sampling with cv |     [.......                                 ] 18% | 6.94 sec | 100 random sampling with cv |     [.......                                 ] 19% | 7.29 sec | 100 random sampling with cv |     [........                                ] 20% | 7.65 sec | 100 random sampling with cv |     [........                                ] 21% | 8.00 sec | 100 random sampling with cv |     [........                                ] 22% | 8.36 sec | 100 random sampling with cv |     [.........                               ] 23% | 8.71 sec | 100 random sampling with cv |     [.........                               ] 24% | 9.07 sec | 100 random sampling with cv |     [..........                              ] 25% | 9.42 sec | 100 random sampling with cv |     [..........                              ] 26% | 9.78 sec | 100 random sampling with cv |     [..........                              ] 27% | 10.13 sec | 100 random sampling with cv |     [...........                             ] 28% | 10.49 sec | 100 random sampling with cv |     [...........                             ] 29% | 10.85 sec | 100 random sampling with cv |     [............                            ] 30% | 11.20 sec | 100 random sampling with cv |     [............                            ] 31% | 11.56 sec | 100 random sampling with cv |     [............                            ] 32% | 11.91 sec | 100 random sampling with cv |     [.............                           ] 33% | 12.27 sec | 100 random sampling with cv |     [.............                           ] 34% | 12.62 sec | 100 random sampling with cv |     [..............                          ] 35% | 12.98 sec | 100 random sampling with cv |     [..............                          ] 36% | 13.33 sec | 100 random sampling with cv |     [..............                          ] 37% | 13.69 sec | 100 random sampling with cv |     [...............                         ] 38% | 14.05 sec | 100 random sampling with cv |     [...............                         ] 39% | 14.40 sec | 100 random sampling with cv |     [................                        ] 40% | 14.76 sec | 100 random sampling with cv |     [................                        ] 41% | 15.11 sec | 100 random sampling with cv |     [................                        ] 42% | 15.47 sec | 100 random sampling with cv |     [.................                       ] 43% | 15.79 sec | 100 random sampling with cv |     [.................                       ] 44% | 16.15 sec | 100 random sampling with cv |     [..................                      ] 45% | 16.50 sec | 100 random sampling with cv |     [..................                      ] 46% | 16.86 sec | 100 random sampling with cv |     [..................                      ] 47% | 17.21 sec | 100 random sampling with cv |     [...................                     ] 48% | 17.57 sec | 100 random sampling with cv |     [...................                     ] 49% | 17.92 sec | 100 random sampling with cv |     [....................                    ] 50% | 18.28 sec | 100 random sampling with cv |     [....................                    ] 51% | 18.64 sec | 100 random sampling with cv |     [....................                    ] 52% | 19.00 sec | 100 random sampling with cv |     [.....................                   ] 53% | 19.35 sec | 100 random sampling with cv |     [.....................                   ] 54% | 19.71 sec | 100 random sampling with cv |     [......................                  ] 55% | 20.06 sec | 100 random sampling with cv |     [......................                  ] 56% | 20.42 sec | 100 random sampling with cv |     [......................                  ] 57% | 20.77 sec | 100 random sampling with cv |     [.......................                 ] 58% | 21.13 sec | 100 random sampling with cv |     [.......................                 ] 59% | 21.47 sec | 100 random sampling with cv |     [........................                ] 60% | 21.83 sec | 100 random sampling with cv |     [........................                ] 61% | 22.18 sec | 100 random sampling with cv |     [........................                ] 62% | 22.54 sec | 100 random sampling with cv |     [.........................               ] 63% | 22.89 sec | 100 random sampling with cv |     [.........................               ] 64% | 23.25 sec | 100 random sampling with cv |     [..........................              ] 65% | 23.61 sec | 100 random sampling with cv |     [..........................              ] 66% | 23.96 sec | 100 random sampling with cv |     [..........................              ] 67% | 24.32 sec | 100 random sampling with cv |     [...........................             ] 68% | 24.68 sec | 100 random sampling with cv |     [...........................             ] 69% | 25.03 sec | 100 random sampling with cv |     [............................            ] 70% | 25.39 sec | 100 random sampling with cv |     [............................            ] 71% | 25.74 sec | 100 random sampling with cv |     [............................            ] 72% | 26.10 sec | 100 random sampling with cv |     [.............................           ] 73% | 26.45 sec | 100 random sampling with cv |     [.............................           ] 74% | 26.81 sec | 100 random sampling with cv |     [..............................          ] 75% | 27.17 sec | 100 random sampling with cv |     [..............................          ] 76% | 27.52 sec | 100 random sampling with cv |     [..............................          ] 77% | 27.88 sec | 100 random sampling with cv |     [...............................         ] 78% | 28.23 sec | 100 random sampling with cv |     [...............................         ] 79% | 28.59 sec | 100 random sampling with cv |     [................................        ] 80% | 28.95 sec | 100 random sampling with cv |     [................................        ] 81% | 29.30 sec | 100 random sampling with cv |     [................................        ] 82% | 29.66 sec | 100 random sampling with cv |     [.................................       ] 83% | 30.01 sec | 100 random sampling with cv |     [.................................       ] 84% | 30.37 sec | 100 random sampling with cv |     [..................................      ] 85% | 30.73 sec | 100 random sampling with cv |     [..................................      ] 86% | 31.08 sec | 100 random sampling with cv |     [..................................      ] 87% | 31.44 sec | 100 random sampling with cv |     [...................................     ] 88% | 31.80 sec | 100 random sampling with cv |     [...................................     ] 89% | 32.15 sec | 100 random sampling with cv |     [....................................    ] 90% | 32.51 sec | 100 random sampling with cv |     [....................................    ] 91% | 32.86 sec | 100 random sampling with cv |     [....................................    ] 92% | 33.22 sec | 100 random sampling with cv |     [.....................................   ] 93% | 33.58 sec | 100 random sampling with cv |     [.....................................   ] 94% | 33.93 sec | 100 random sampling with cv |     [......................................  ] 95% | 34.29 sec | 100 random sampling with cv |     [......................................  ] 96% | 34.65 sec | 100 random sampling with cv |     [......................................  ] 97% | 35.00 sec | 100 random sampling with cv |     [....................................... ] 98% | 35.36 sec | 100 random sampling with cv |     [....................................... ] 99% | 35.71 sec | 100 random sampling with cv |     [........................................] 100% | 36.07 sec | 100 random sampling with cv | 

    Pipeline(steps=[('columnkernelizer',
                     ColumnKernelizer(transformers=[('feature space A',
                                                     Kernelizer(),
                                                     slice(0, 1000, None)),
                                                    ('feature space B',
                                                     Kernelizer(),
                                                     slice(1000, 2000, None)),
                                                    ('feature space C',
                                                     Kernelizer(),
                                                     slice(2000, 2500, None))])),
                    ('multiplekernelridgecv',
                     MultipleKernelRidgeCV(kernels='precomputed',
                                           solver_params={'alphas': array([1.00000000e-10, 3.1622776...
           1.00000000e+02, 3.16227766e+02, 1.00000000e+03, 3.16227766e+03,
           1.00000000e+04, 3.16227766e+04, 1.00000000e+05, 3.16227766e+05,
           1.00000000e+06, 3.16227766e+06, 1.00000000e+07, 3.16227766e+07,
           1.00000000e+08, 3.16227766e+08, 1.00000000e+09, 3.16227766e+09,
           1.00000000e+10]),
                                                          'jitter_alphas': True,
                                                          'n_alphas_batch': 20,
                                                          'n_iter': 100,
                                                          'n_targets_batch': 1000,
                                                          'n_targets_batch_refit': 200}))])



.. GENERATED FROM PYTHON SOURCE LINES 143-145

Plot the convergence curve
--------------------------

.. GENERATED FROM PYTHON SOURCE LINES 145-161

.. code-block:: default


    # ``cv_scores`` gives the scores for each sampled kernel weights.
    # The convergence curve is thus the current maximum for each target.
    cv_scores = backend.to_numpy(pipe[1].cv_scores_)
    current_max = np.maximum.accumulate(cv_scores, axis=0)
    mean_current_max = np.mean(current_max, axis=1)

    x_array = np.arange(1, len(mean_current_max) + 1)
    plt.plot(x_array, mean_current_max, '-o')
    plt.grid("on")
    plt.xlabel("Number of kernel weights sampled")
    plt.ylabel("L2 negative loss (higher is better)")
    plt.title("Convergence curve, averaged over targets")
    plt.tight_layout()
    plt.show()




.. image:: /_auto_examples/multiple_kernel_ridge/images/sphx_glr_plot_mkr_sklearn_api_001.png
    :alt: Convergence curve, averaged over targets
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 162-166

Compare to ``KernelRidgeCV``
----------------------------
Compare to a baseline ``KernelRidgeCV`` model with all the concatenated features.
Comparison is performed using the prediction scores on the test set.

.. GENERATED FROM PYTHON SOURCE LINES 169-170

Fit the baseline model ``KernelRidgeCV``

.. GENERATED FROM PYTHON SOURCE LINES 170-173

.. code-block:: default

    baseline = KernelRidgeCV(kernel="linear", alphas=alphas)
    baseline.fit(X_train, Y_train)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    KernelRidgeCV(alphas=array([1.00000000e-10, 3.16227766e-10, 1.00000000e-09, 3.16227766e-09,
           1.00000000e-08, 3.16227766e-08, 1.00000000e-07, 3.16227766e-07,
           1.00000000e-06, 3.16227766e-06, 1.00000000e-05, 3.16227766e-05,
           1.00000000e-04, 3.16227766e-04, 1.00000000e-03, 3.16227766e-03,
           1.00000000e-02, 3.16227766e-02, 1.00000000e-01, 3.16227766e-01,
           1.00000000e+00, 3.16227766e+00, 1.00000000e+01, 3.16227766e+01,
           1.00000000e+02, 3.16227766e+02, 1.00000000e+03, 3.16227766e+03,
           1.00000000e+04, 3.16227766e+04, 1.00000000e+05, 3.16227766e+05,
           1.00000000e+06, 3.16227766e+06, 1.00000000e+07, 3.16227766e+07,
           1.00000000e+08, 3.16227766e+08, 1.00000000e+09, 3.16227766e+09,
           1.00000000e+10]))



.. GENERATED FROM PYTHON SOURCE LINES 174-175

Compute scores of both models

.. GENERATED FROM PYTHON SOURCE LINES 175-181

.. code-block:: default

    scores = pipe.score(X_test, Y_test)
    scores = backend.to_numpy(scores)

    scores_baseline = baseline.score(X_test, Y_test)
    scores_baseline = backend.to_numpy(scores_baseline)








.. GENERATED FROM PYTHON SOURCE LINES 182-183

Plot histograms

.. GENERATED FROM PYTHON SOURCE LINES 183-191

.. code-block:: default

    bins = np.linspace(min(scores_baseline.min(), scores.min()),
                       max(scores_baseline.max(), scores.max()), 50)
    plt.hist(scores, bins, alpha=0.5, label="MultipleKernelRidgeCV")
    plt.hist(scores_baseline, bins, alpha=0.5, label="KernelRidgeCV")
    plt.xlabel(r"$R^2$ generalization score")
    plt.title("Histogram over targets")
    plt.legend()
    plt.show()



.. image:: /_auto_examples/multiple_kernel_ridge/images/sphx_glr_plot_mkr_sklearn_api_002.png
    :alt: Histogram over targets
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  40.430 seconds)


.. _sphx_glr_download__auto_examples_multiple_kernel_ridge_plot_mkr_sklearn_api.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_mkr_sklearn_api.py <plot_mkr_sklearn_api.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_mkr_sklearn_api.ipynb <plot_mkr_sklearn_api.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
