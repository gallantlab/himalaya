
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_auto_examples/multiple_kernel_ridge/plot_mkr_random_search.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download__auto_examples_multiple_kernel_ridge_plot_mkr_random_search.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__auto_examples_multiple_kernel_ridge_plot_mkr_random_search.py:


Multiple kernel ridge regression
================================
This example demonstrates how to solve multiple kernel ridge regression.
It uses random search and cross validation to select optimal hyperparameters.

.. GENERATED FROM PYTHON SOURCE LINES 7-17

.. code-block:: default


    import numpy as np
    import matplotlib.pyplot as plt

    from himalaya.backend import set_backend
    from himalaya.kernel_ridge import solve_multiple_kernel_ridge_random_search
    from himalaya.kernel_ridge import predict_and_score_weighted_kernel_ridge
    from himalaya.scoring import r2_score_split
    from himalaya.viz import plot_alphas_diagnostic








.. GENERATED FROM PYTHON SOURCE LINES 19-20

In this example, we use the ``cupy`` backend, and fit the model on GPU.

.. GENERATED FROM PYTHON SOURCE LINES 20-23

.. code-block:: default


    backend = set_backend("cupy")








.. GENERATED FROM PYTHON SOURCE LINES 24-31

Generate a random dataset
-------------------------

- Xs_train : list of arrays of shape (n_samples_train, n_features)
- Xs_test : list of arrays of shape (n_samples_test, n_features)
- Y_train : array of shape (n_samples_train, n_targets)
- Y_test : array of shape (n_repeat, n_samples_test, n_targets)

.. GENERATED FROM PYTHON SOURCE LINES 31-51

.. code-block:: default


    n_samples_train = 1000
    n_samples_test = 300
    n_targets = 1000
    n_features_list = [1000, 1000, 500]

    Xs_train = [
        backend.randn(n_samples_train, n_features)
        for n_features in n_features_list
    ]
    Xs_test = [
        backend.randn(n_samples_test, n_features) for n_features in n_features_list
    ]
    ws = [
        backend.randn(n_features, n_targets) / n_features
        for n_features in n_features_list
    ]
    Y_train = backend.stack([X @ w for X, w in zip(Xs_train, ws)]).sum(0)
    Y_test = backend.stack([X @ w for X, w in zip(Xs_test, ws)]).sum(0)








.. GENERATED FROM PYTHON SOURCE LINES 52-53

Optional: Add some arbitrary scalings per kernel

.. GENERATED FROM PYTHON SOURCE LINES 53-61

.. code-block:: default

    if True:
        scalings = [0.2, 5, 1]
        Xs_train = [X * scaling for X, scaling in zip(Xs_train, scalings)]
        Xs_test = [X * scaling for X, scaling in zip(Xs_test, scalings)]

    Y_train -= Y_train.mean(0)
    Y_test -= Y_test.mean(0)








.. GENERATED FROM PYTHON SOURCE LINES 62-65

Precompute the linear kernels
-----------------------------
We also cast them to float32.

.. GENERATED FROM PYTHON SOURCE LINES 65-75

.. code-block:: default


    Ks_train = backend.stack([X_train @ X_train.T for X_train in Xs_train])
    Ks_train = backend.asarray(Ks_train, dtype=backend.float32)
    Y_train = backend.asarray(Y_train, dtype=backend.float32)

    Ks_test = backend.stack(
        [X_test @ X_train.T for X_train, X_test in zip(Xs_train, Xs_test)])
    Ks_test = backend.asarray(Ks_test, dtype=backend.float32)
    Y_test = backend.asarray(Y_test, dtype=backend.float32)








.. GENERATED FROM PYTHON SOURCE LINES 76-81

Run the solver, using random search
-----------------------------------
This method should work fine for
small number of kernels (< 20). The larger the number of kenels, the larger
we need to sample the hyperparameter space (i.e. increasing ``n_iter``).

.. GENERATED FROM PYTHON SOURCE LINES 83-86

Here we use 100 iterations to have a reasonably fast example (~40 sec).
To have a better convergence, we probably need more iterations.
Note that there is currently no stopping criterion in this method.

.. GENERATED FROM PYTHON SOURCE LINES 86-88

.. code-block:: default

    n_iter = 100








.. GENERATED FROM PYTHON SOURCE LINES 89-90

Grid of regularization parameters.

.. GENERATED FROM PYTHON SOURCE LINES 90-92

.. code-block:: default

    alphas = np.logspace(-10, 10, 21)








.. GENERATED FROM PYTHON SOURCE LINES 93-96

Batch parameters are used to reduce the necessary GPU memory. A larger value
will be a bit faster, but the solver might crash if it runs out of memory.
Optimal values depend on the size of your dataset.

.. GENERATED FROM PYTHON SOURCE LINES 96-99

.. code-block:: default

    n_targets_batch = 1000
    n_alphas_batch = 20








.. GENERATED FROM PYTHON SOURCE LINES 100-104

If ``return_weights == "dual"``, the solver will use more memory.
To mitigate this, you can reduce ``n_targets_batch`` in the refit
using ```n_targets_batch_refit``.
If you don't need the dual weights, use ``return_weights = None``.

.. GENERATED FROM PYTHON SOURCE LINES 104-107

.. code-block:: default

    return_weights = 'dual'
    n_targets_batch_refit = 200








.. GENERATED FROM PYTHON SOURCE LINES 108-119

Run the solver. For each iteration, it will:

- sample kernel weights from a Dirichlet distribution
- fit (n_splits * n_alphas * n_targets) ridge models
- compute the scores on the validation set of each split
- average the scores over splits
- take the maximum over alphas
- (only if you ask for the ridge weights) refit using the best alphas per
  target and the entire dataset
- return for each target the log kernel weights leading to the best CV score
  (and the best weights if necessary)

.. GENERATED FROM PYTHON SOURCE LINES 119-131

.. code-block:: default

    results = solve_multiple_kernel_ridge_random_search(
        Ks=Ks_train,
        Y=Y_train,
        n_iter=n_iter,
        alphas=alphas,
        n_targets_batch=n_targets_batch,
        return_weights=return_weights,
        n_alphas_batch=n_alphas_batch,
        n_targets_batch_refit=n_targets_batch_refit,
        jitter_alphas=True,
    )





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [                                        ] 0% | 0.00 sec | 100 random sampling with cv |     [                                        ] 1% | 0.45 sec | 100 random sampling with cv |     [                                        ] 2% | 0.82 sec | 100 random sampling with cv |     [.                                       ] 3% | 1.11 sec | 100 random sampling with cv |     [.                                       ] 4% | 1.40 sec | 100 random sampling with cv |     [..                                      ] 5% | 1.76 sec | 100 random sampling with cv |     [..                                      ] 6% | 2.05 sec | 100 random sampling with cv |     [..                                      ] 7% | 2.34 sec | 100 random sampling with cv |     [...                                     ] 8% | 2.63 sec | 100 random sampling with cv |     [...                                     ] 9% | 2.92 sec | 100 random sampling with cv |     [....                                    ] 10% | 3.21 sec | 100 random sampling with cv |     [....                                    ] 11% | 3.50 sec | 100 random sampling with cv |     [....                                    ] 12% | 3.79 sec | 100 random sampling with cv |     [.....                                   ] 13% | 4.08 sec | 100 random sampling with cv |     [.....                                   ] 14% | 4.37 sec | 100 random sampling with cv |     [......                                  ] 15% | 4.66 sec | 100 random sampling with cv |     [......                                  ] 16% | 4.95 sec | 100 random sampling with cv |     [......                                  ] 17% | 5.24 sec | 100 random sampling with cv |     [.......                                 ] 18% | 5.53 sec | 100 random sampling with cv |     [.......                                 ] 19% | 5.82 sec | 100 random sampling with cv |     [........                                ] 20% | 6.11 sec | 100 random sampling with cv |     [........                                ] 21% | 6.40 sec | 100 random sampling with cv |     [........                                ] 22% | 6.69 sec | 100 random sampling with cv |     [.........                               ] 23% | 7.03 sec | 100 random sampling with cv |     [.........                               ] 24% | 7.32 sec | 100 random sampling with cv |     [..........                              ] 25% | 7.61 sec | 100 random sampling with cv |     [..........                              ] 26% | 7.90 sec | 100 random sampling with cv |     [..........                              ] 27% | 8.19 sec | 100 random sampling with cv |     [...........                             ] 28% | 8.47 sec | 100 random sampling with cv |     [...........                             ] 29% | 8.76 sec | 100 random sampling with cv |     [............                            ] 30% | 9.05 sec | 100 random sampling with cv |     [............                            ] 31% | 9.38 sec | 100 random sampling with cv |     [............                            ] 32% | 9.67 sec | 100 random sampling with cv |     [.............                           ] 33% | 9.96 sec | 100 random sampling with cv |     [.............                           ] 34% | 10.25 sec | 100 random sampling with cv |     [..............                          ] 35% | 10.54 sec | 100 random sampling with cv |     [..............                          ] 36% | 10.83 sec | 100 random sampling with cv |     [..............                          ] 37% | 11.19 sec | 100 random sampling with cv |     [...............                         ] 38% | 11.47 sec | 100 random sampling with cv |     [...............                         ] 39% | 11.76 sec | 100 random sampling with cv |     [................                        ] 40% | 12.05 sec | 100 random sampling with cv |     [................                        ] 41% | 12.34 sec | 100 random sampling with cv |     [................                        ] 42% | 12.63 sec | 100 random sampling with cv |     [.................                       ] 43% | 12.92 sec | 100 random sampling with cv |     [.................                       ] 44% | 13.21 sec | 100 random sampling with cv |     [..................                      ] 45% | 13.50 sec | 100 random sampling with cv |     [..................                      ] 46% | 13.79 sec | 100 random sampling with cv |     [..................                      ] 47% | 14.08 sec | 100 random sampling with cv |     [...................                     ] 48% | 14.37 sec | 100 random sampling with cv |     [...................                     ] 49% | 14.66 sec | 100 random sampling with cv |     [....................                    ] 50% | 14.95 sec | 100 random sampling with cv |     [....................                    ] 51% | 15.30 sec | 100 random sampling with cv |     [....................                    ] 52% | 15.59 sec | 100 random sampling with cv |     [.....................                   ] 53% | 15.88 sec | 100 random sampling with cv |     [.....................                   ] 54% | 16.17 sec | 100 random sampling with cv |     [......................                  ] 55% | 16.52 sec | 100 random sampling with cv |     [......................                  ] 56% | 16.81 sec | 100 random sampling with cv |     [......................                  ] 57% | 17.16 sec | 100 random sampling with cv |     [.......................                 ] 58% | 17.45 sec | 100 random sampling with cv |     [.......................                 ] 59% | 17.72 sec | 100 random sampling with cv |     [........................                ] 60% | 18.01 sec | 100 random sampling with cv |     [........................                ] 61% | 18.30 sec | 100 random sampling with cv |     [........................                ] 62% | 18.59 sec | 100 random sampling with cv |     [.........................               ] 63% | 18.94 sec | 100 random sampling with cv |     [.........................               ] 64% | 19.24 sec | 100 random sampling with cv |     [..........................              ] 65% | 19.53 sec | 100 random sampling with cv |     [..........................              ] 66% | 19.82 sec | 100 random sampling with cv |     [..........................              ] 67% | 20.11 sec | 100 random sampling with cv |     [...........................             ] 68% | 20.40 sec | 100 random sampling with cv |     [...........................             ] 69% | 20.74 sec | 100 random sampling with cv |     [............................            ] 70% | 21.03 sec | 100 random sampling with cv |     [............................            ] 71% | 21.32 sec | 100 random sampling with cv |     [............................            ] 72% | 21.61 sec | 100 random sampling with cv |     [.............................           ] 73% | 21.98 sec | 100 random sampling with cv |     [.............................           ] 74% | 22.27 sec | 100 random sampling with cv |     [..............................          ] 75% | 22.63 sec | 100 random sampling with cv |     [..............................          ] 76% | 22.92 sec | 100 random sampling with cv |     [..............................          ] 77% | 23.21 sec | 100 random sampling with cv |     [...............................         ] 78% | 23.50 sec | 100 random sampling with cv |     [...............................         ] 79% | 23.79 sec | 100 random sampling with cv |     [................................        ] 80% | 24.08 sec | 100 random sampling with cv |     [................................        ] 81% | 24.45 sec | 100 random sampling with cv |     [................................        ] 82% | 24.74 sec | 100 random sampling with cv |     [.................................       ] 83% | 25.03 sec | 100 random sampling with cv |     [.................................       ] 84% | 25.32 sec | 100 random sampling with cv |     [..................................      ] 85% | 25.61 sec | 100 random sampling with cv |     [..................................      ] 86% | 25.90 sec | 100 random sampling with cv |     [..................................      ] 87% | 26.17 sec | 100 random sampling with cv |     [...................................     ] 88% | 26.46 sec | 100 random sampling with cv |     [...................................     ] 89% | 26.75 sec | 100 random sampling with cv |     [....................................    ] 90% | 27.04 sec | 100 random sampling with cv |     [....................................    ] 91% | 27.33 sec | 100 random sampling with cv |     [....................................    ] 92% | 27.62 sec | 100 random sampling with cv |     [.....................................   ] 93% | 27.91 sec | 100 random sampling with cv |     [.....................................   ] 94% | 28.27 sec | 100 random sampling with cv |     [......................................  ] 95% | 28.56 sec | 100 random sampling with cv |     [......................................  ] 96% | 28.85 sec | 100 random sampling with cv |     [......................................  ] 97% | 29.21 sec | 100 random sampling with cv |     [....................................... ] 98% | 29.51 sec | 100 random sampling with cv |     [....................................... ] 99% | 29.80 sec | 100 random sampling with cv |     [........................................] 100% | 30.10 sec | 100 random sampling with cv | 




.. GENERATED FROM PYTHON SOURCE LINES 132-134

As we used the ``cupy`` backend, the results are ``cupy`` arrays, which are
on GPU. Here, we cast the results back to CPU, and to ``numpy`` arrays.

.. GENERATED FROM PYTHON SOURCE LINES 134-138

.. code-block:: default

    deltas = backend.to_numpy(results[0])
    dual_weights = backend.to_numpy(results[1])
    cv_scores = backend.to_numpy(results[2])








.. GENERATED FROM PYTHON SOURCE LINES 139-144

Plot the convergence curve
--------------------------

``cv_scores`` gives the scores for each sampled kernel weights.
The convergence curve is thus the current maximum for each target.

.. GENERATED FROM PYTHON SOURCE LINES 144-156

.. code-block:: default


    current_max = np.maximum.accumulate(cv_scores, axis=0)
    mean_current_max = np.mean(current_max, axis=1)
    x_array = np.arange(1, len(mean_current_max) + 1)
    plt.plot(x_array, mean_current_max, '-o')
    plt.grid("on")
    plt.xlabel("Number of kernel weights sampled")
    plt.ylabel("L2 negative loss (higher is better)")
    plt.title("Convergence curve, averaged over targets")
    plt.tight_layout()
    plt.show()




.. image:: /_auto_examples/multiple_kernel_ridge/images/sphx_glr_plot_mkr_random_search_001.png
    :alt: Convergence curve, averaged over targets
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 157-162

Plot the optimal alphas selected by the solver
----------------------------------------------

This plot is helpful to refine the alpha grid if the range is too small or
too large.

.. GENERATED FROM PYTHON SOURCE LINES 162-168

.. code-block:: default


    best_alphas = 1. / np.sum(np.exp(deltas), axis=0)
    plot_alphas_diagnostic(best_alphas, alphas)
    plt.title("Best alphas selected by cross-validation")
    plt.show()




.. image:: /_auto_examples/multiple_kernel_ridge/images/sphx_glr_plot_mkr_random_search_002.png
    :alt: Best alphas selected by cross-validation
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 169-172

Compute the predictions on the test set
---------------------------------------
(requires the dual weights)

.. GENERATED FROM PYTHON SOURCE LINES 172-184

.. code-block:: default


    split = False
    scores = predict_and_score_weighted_kernel_ridge(
        Ks_test, dual_weights, deltas, Y_test, split=split,
        n_targets_batch=n_targets_batch, score_func=r2_score_split)
    scores = backend.to_numpy(scores)

    plt.hist(scores, 50)
    plt.xlabel(r"$R^2$ generalization score")
    plt.title("Histogram over targets")
    plt.show()




.. image:: /_auto_examples/multiple_kernel_ridge/images/sphx_glr_plot_mkr_random_search_003.png
    :alt: Histogram over targets
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 185-192

Compute the split predictions on the test set 
---------------------------------------------
(requires the dual weights)

Here we apply the dual weights on each kernel separately
(``exp(deltas[i]) * kernel[i]``), and we compute the R\ :sup:`2` scores
(corrected for correlations) of each prediction.

.. GENERATED FROM PYTHON SOURCE LINES 192-205

.. code-block:: default


    split = True
    scores = predict_and_score_weighted_kernel_ridge(
        Ks_test, dual_weights, deltas, Y_test, split=split,
        n_targets_batch=n_targets_batch, score_func=r2_score_split)
    scores = backend.to_numpy(scores)

    bins = np.linspace(scores.min(), scores.max(), 50)
    for score in scores:
        plt.hist(score, bins, alpha=0.5)
    plt.title(r"Histogram of $R^2$ generalization score split between kernels")
    plt.legend(["kernel %d" % kk for kk in range(scores.shape[0])])
    plt.show()



.. image:: /_auto_examples/multiple_kernel_ridge/images/sphx_glr_plot_mkr_random_search_004.png
    :alt: Histogram of $R^2$ generalization score split between kernels
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  30.633 seconds)


.. _sphx_glr_download__auto_examples_multiple_kernel_ridge_plot_mkr_random_search.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_mkr_random_search.py <plot_mkr_random_search.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_mkr_random_search.ipynb <plot_mkr_random_search.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
