
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_auto_examples/multiple_kernel_ridge/plot_mkr_0_random_search.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download__auto_examples_multiple_kernel_ridge_plot_mkr_0_random_search.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__auto_examples_multiple_kernel_ridge_plot_mkr_0_random_search.py:


Multiple-kernel ridge
=====================
This example demonstrates how to solve multiple kernel ridge regression.
It uses random search and cross validation to select optimal hyperparameters.

.. GENERATED FROM PYTHON SOURCE LINES 7-18

.. code-block:: default


    import numpy as np
    import matplotlib.pyplot as plt

    from himalaya.backend import set_backend
    from himalaya.kernel_ridge import solve_multiple_kernel_ridge_random_search
    from himalaya.kernel_ridge import predict_and_score_weighted_kernel_ridge
    from himalaya.utils import generate_multikernel_dataset
    from himalaya.scoring import r2_score_split
    from himalaya.viz import plot_alphas_diagnostic








.. GENERATED FROM PYTHON SOURCE LINES 20-21

In this example, we use the ``cupy`` backend, and fit the model on GPU.

.. GENERATED FROM PYTHON SOURCE LINES 21-24

.. code-block:: default


    backend = set_backend("cupy")








.. GENERATED FROM PYTHON SOURCE LINES 25-32

Generate a random dataset
-------------------------

- X_train : array of shape (n_samples_train, n_features)
- X_test : array of shape (n_samples_test, n_features)
- Y_train : array of shape (n_samples_train, n_targets)
- Y_test : array of shape (n_samples_test, n_targets)

.. GENERATED FROM PYTHON SOURCE LINES 32-54

.. code-block:: default


    n_kernels = 3
    n_targets = 500
    kernel_weights_true = np.tile(np.array([0.5, 0.3, 0.2])[None], (n_targets, 1))

    (X_train, X_test, Y_train, Y_test, kernel_weights_true,
     n_features_list) = generate_multikernel_dataset(
         n_kernels=n_kernels, n_targets=n_targets, n_samples_train=1000,
         n_samples_test=300, kernel_weights_true=kernel_weights_true,
         random_state=42)

    feature_names = [f"Feature space {ii}" for ii in range(len(n_features_list))]

    # Find the start and end of each feature space X in Xs
    start_and_end = np.concatenate([[0], np.cumsum(n_features_list)])
    slices = [
        slice(start, end)
        for start, end in zip(start_and_end[:-1], start_and_end[1:])
    ]
    Xs_train = [X_train[:, slic] for slic in slices]
    Xs_test = [X_test[:, slic] for slic in slices]








.. GENERATED FROM PYTHON SOURCE LINES 55-58

Precompute the linear kernels
-----------------------------
We also cast them to float32.

.. GENERATED FROM PYTHON SOURCE LINES 58-68

.. code-block:: default


    Ks_train = backend.stack([X_train @ X_train.T for X_train in Xs_train])
    Ks_train = backend.asarray(Ks_train, dtype=backend.float32)
    Y_train = backend.asarray(Y_train, dtype=backend.float32)

    Ks_test = backend.stack(
        [X_test @ X_train.T for X_train, X_test in zip(Xs_train, Xs_test)])
    Ks_test = backend.asarray(Ks_test, dtype=backend.float32)
    Y_test = backend.asarray(Y_test, dtype=backend.float32)








.. GENERATED FROM PYTHON SOURCE LINES 69-74

Run the solver, using random search
-----------------------------------
This method should work fine for
small number of kernels (< 20). The larger the number of kenels, the larger
we need to sample the hyperparameter space (i.e. increasing ``n_iter``).

.. GENERATED FROM PYTHON SOURCE LINES 76-79

Here we use 100 iterations to have a reasonably fast example (~40 sec).
To have a better convergence, we probably need more iterations.
Note that there is currently no stopping criterion in this method.

.. GENERATED FROM PYTHON SOURCE LINES 79-81

.. code-block:: default

    n_iter = 100








.. GENERATED FROM PYTHON SOURCE LINES 82-83

Grid of regularization parameters.

.. GENERATED FROM PYTHON SOURCE LINES 83-85

.. code-block:: default

    alphas = np.logspace(-10, 10, 21)








.. GENERATED FROM PYTHON SOURCE LINES 86-89

Batch parameters are used to reduce the necessary GPU memory. A larger value
will be a bit faster, but the solver might crash if it runs out of memory.
Optimal values depend on the size of your dataset.

.. GENERATED FROM PYTHON SOURCE LINES 89-92

.. code-block:: default

    n_targets_batch = 1000
    n_alphas_batch = 20








.. GENERATED FROM PYTHON SOURCE LINES 93-97

If ``return_weights == "dual"``, the solver will use more memory.
To mitigate this, you can reduce ``n_targets_batch`` in the refit
using ```n_targets_batch_refit``.
If you don't need the dual weights, use ``return_weights = None``.

.. GENERATED FROM PYTHON SOURCE LINES 97-100

.. code-block:: default

    return_weights = 'dual'
    n_targets_batch_refit = 200








.. GENERATED FROM PYTHON SOURCE LINES 101-112

Run the solver. For each iteration, it will:

- sample kernel weights from a Dirichlet distribution
- fit (n_splits * n_alphas * n_targets) ridge models
- compute the scores on the validation set of each split
- average the scores over splits
- take the maximum over alphas
- (only if you ask for the ridge weights) refit using the best alphas per
  target and the entire dataset
- return for each target the log kernel weights leading to the best CV score
  (and the best weights if necessary)

.. GENERATED FROM PYTHON SOURCE LINES 112-124

.. code-block:: default

    results = solve_multiple_kernel_ridge_random_search(
        Ks=Ks_train,
        Y=Y_train,
        n_iter=n_iter,
        alphas=alphas,
        n_targets_batch=n_targets_batch,
        return_weights=return_weights,
        n_alphas_batch=n_alphas_batch,
        n_targets_batch_refit=n_targets_batch_refit,
        jitter_alphas=True,
    )





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [                                        ] 0% | 0.00 sec | 100 random sampling with cv |     [                                        ] 1% | 0.43 sec | 100 random sampling with cv |     [                                        ] 2% | 0.79 sec | 100 random sampling with cv |     [.                                       ] 3% | 1.07 sec | 100 random sampling with cv |     [.                                       ] 4% | 1.42 sec | 100 random sampling with cv |     [..                                      ] 5% | 1.70 sec | 100 random sampling with cv |     [..                                      ] 6% | 1.98 sec | 100 random sampling with cv |     [..                                      ] 7% | 2.26 sec | 100 random sampling with cv |     [...                                     ] 8% | 2.60 sec | 100 random sampling with cv |     [...                                     ] 9% | 2.88 sec | 100 random sampling with cv |     [....                                    ] 10% | 3.23 sec | 100 random sampling with cv |     [....                                    ] 11% | 3.51 sec | 100 random sampling with cv |     [....                                    ] 12% | 3.86 sec | 100 random sampling with cv |     [.....                                   ] 13% | 4.14 sec | 100 random sampling with cv |     [.....                                   ] 14% | 4.42 sec | 100 random sampling with cv |     [......                                  ] 15% | 4.69 sec | 100 random sampling with cv |     [......                                  ] 16% | 4.97 sec | 100 random sampling with cv |     [......                                  ] 17% | 5.25 sec | 100 random sampling with cv |     [.......                                 ] 18% | 5.53 sec | 100 random sampling with cv |     [.......                                 ] 19% | 5.81 sec | 100 random sampling with cv |     [........                                ] 20% | 6.09 sec | 100 random sampling with cv |     [........                                ] 21% | 6.37 sec | 100 random sampling with cv |     [........                                ] 22% | 6.64 sec | 100 random sampling with cv |     [.........                               ] 23% | 6.92 sec | 100 random sampling with cv |     [.........                               ] 24% | 7.20 sec | 100 random sampling with cv |     [..........                              ] 25% | 7.48 sec | 100 random sampling with cv |     [..........                              ] 26% | 7.76 sec | 100 random sampling with cv |     [..........                              ] 27% | 8.04 sec | 100 random sampling with cv |     [...........                             ] 28% | 8.38 sec | 100 random sampling with cv |     [...........                             ] 29% | 8.66 sec | 100 random sampling with cv |     [............                            ] 30% | 9.02 sec | 100 random sampling with cv |     [............                            ] 31% | 9.30 sec | 100 random sampling with cv |     [............                            ] 32% | 9.58 sec | 100 random sampling with cv |     [.............                           ] 33% | 9.86 sec | 100 random sampling with cv |     [.............                           ] 34% | 10.14 sec | 100 random sampling with cv |     [..............                          ] 35% | 10.42 sec | 100 random sampling with cv |     [..............                          ] 36% | 10.70 sec | 100 random sampling with cv |     [..............                          ] 37% | 10.98 sec | 100 random sampling with cv |     [...............                         ] 38% | 11.33 sec | 100 random sampling with cv |     [...............                         ] 39% | 11.61 sec | 100 random sampling with cv |     [................                        ] 40% | 11.89 sec | 100 random sampling with cv |     [................                        ] 41% | 12.17 sec | 100 random sampling with cv |     [................                        ] 42% | 12.45 sec | 100 random sampling with cv |     [.................                       ] 43% | 12.80 sec | 100 random sampling with cv |     [.................                       ] 44% | 13.07 sec | 100 random sampling with cv |     [..................                      ] 45% | 13.35 sec | 100 random sampling with cv |     [..................                      ] 46% | 13.70 sec | 100 random sampling with cv |     [..................                      ] 47% | 13.98 sec | 100 random sampling with cv |     [...................                     ] 48% | 14.26 sec | 100 random sampling with cv |     [...................                     ] 49% | 14.54 sec | 100 random sampling with cv |     [....................                    ] 50% | 14.82 sec | 100 random sampling with cv |     [....................                    ] 51% | 15.10 sec | 100 random sampling with cv |     [....................                    ] 52% | 15.45 sec | 100 random sampling with cv |     [.....................                   ] 53% | 15.74 sec | 100 random sampling with cv |     [.....................                   ] 54% | 16.08 sec | 100 random sampling with cv |     [......................                  ] 55% | 16.36 sec | 100 random sampling with cv |     [......................                  ] 56% | 16.64 sec | 100 random sampling with cv |     [......................                  ] 57% | 16.92 sec | 100 random sampling with cv |     [.......................                 ] 58% | 17.20 sec | 100 random sampling with cv |     [.......................                 ] 59% | 17.48 sec | 100 random sampling with cv |     [........................                ] 60% | 17.76 sec | 100 random sampling with cv |     [........................                ] 61% | 18.04 sec | 100 random sampling with cv |     [........................                ] 62% | 18.32 sec | 100 random sampling with cv |     [.........................               ] 63% | 18.60 sec | 100 random sampling with cv |     [.........................               ] 64% | 18.95 sec | 100 random sampling with cv |     [..........................              ] 65% | 19.23 sec | 100 random sampling with cv |     [..........................              ] 66% | 19.51 sec | 100 random sampling with cv |     [..........................              ] 67% | 19.79 sec | 100 random sampling with cv |     [...........................             ] 68% | 20.07 sec | 100 random sampling with cv |     [...........................             ] 69% | 20.35 sec | 100 random sampling with cv |     [............................            ] 70% | 20.63 sec | 100 random sampling with cv |     [............................            ] 71% | 20.91 sec | 100 random sampling with cv |     [............................            ] 72% | 21.19 sec | 100 random sampling with cv |     [.............................           ] 73% | 21.47 sec | 100 random sampling with cv |     [.............................           ] 74% | 21.75 sec | 100 random sampling with cv |     [..............................          ] 75% | 22.03 sec | 100 random sampling with cv |     [..............................          ] 76% | 22.31 sec | 100 random sampling with cv |     [..............................          ] 77% | 22.59 sec | 100 random sampling with cv |     [...............................         ] 78% | 22.87 sec | 100 random sampling with cv |     [...............................         ] 79% | 23.15 sec | 100 random sampling with cv |     [................................        ] 80% | 23.43 sec | 100 random sampling with cv |     [................................        ] 81% | 23.71 sec | 100 random sampling with cv |     [................................        ] 82% | 23.99 sec | 100 random sampling with cv |     [.................................       ] 83% | 24.27 sec | 100 random sampling with cv |     [.................................       ] 84% | 24.55 sec | 100 random sampling with cv |     [..................................      ] 85% | 24.83 sec | 100 random sampling with cv |     [..................................      ] 86% | 25.11 sec | 100 random sampling with cv |     [..................................      ] 87% | 25.39 sec | 100 random sampling with cv |     [...................................     ] 88% | 25.74 sec | 100 random sampling with cv |     [...................................     ] 89% | 26.02 sec | 100 random sampling with cv |     [....................................    ] 90% | 26.30 sec | 100 random sampling with cv |     [....................................    ] 91% | 26.58 sec | 100 random sampling with cv |     [....................................    ] 92% | 26.86 sec | 100 random sampling with cv |     [.....................................   ] 93% | 27.14 sec | 100 random sampling with cv |     [.....................................   ] 94% | 27.42 sec | 100 random sampling with cv |     [......................................  ] 95% | 27.70 sec | 100 random sampling with cv |     [......................................  ] 96% | 27.98 sec | 100 random sampling with cv |     [......................................  ] 97% | 28.26 sec | 100 random sampling with cv |     [....................................... ] 98% | 28.61 sec | 100 random sampling with cv |     [....................................... ] 99% | 28.89 sec | 100 random sampling with cv |     [........................................] 100% | 29.17 sec | 100 random sampling with cv | 




.. GENERATED FROM PYTHON SOURCE LINES 125-127

As we used the ``cupy`` backend, the results are ``cupy`` arrays, which are
on GPU. Here, we cast the results back to CPU, and to ``numpy`` arrays.

.. GENERATED FROM PYTHON SOURCE LINES 127-131

.. code-block:: default

    deltas = backend.to_numpy(results[0])
    dual_weights = backend.to_numpy(results[1])
    cv_scores = backend.to_numpy(results[2])








.. GENERATED FROM PYTHON SOURCE LINES 132-137

Plot the convergence curve
--------------------------

``cv_scores`` gives the scores for each sampled kernel weights.
The convergence curve is thus the current maximum for each target.

.. GENERATED FROM PYTHON SOURCE LINES 137-149

.. code-block:: default


    current_max = np.maximum.accumulate(cv_scores, axis=0)
    mean_current_max = np.mean(current_max, axis=1)
    x_array = np.arange(1, len(mean_current_max) + 1)
    plt.plot(x_array, mean_current_max, '-o')
    plt.grid("on")
    plt.xlabel("Number of kernel weights sampled")
    plt.ylabel("L2 negative loss (higher is better)")
    plt.title("Convergence curve, averaged over targets")
    plt.tight_layout()
    plt.show()




.. image:: /_auto_examples/multiple_kernel_ridge/images/sphx_glr_plot_mkr_0_random_search_001.png
    :alt: Convergence curve, averaged over targets
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 150-155

Plot the optimal alphas selected by the solver
----------------------------------------------

This plot is helpful to refine the alpha grid if the range is too small or
too large.

.. GENERATED FROM PYTHON SOURCE LINES 155-161

.. code-block:: default


    best_alphas = 1. / np.sum(np.exp(deltas), axis=0)
    plot_alphas_diagnostic(best_alphas, alphas)
    plt.title("Best alphas selected by cross-validation")
    plt.show()




.. image:: /_auto_examples/multiple_kernel_ridge/images/sphx_glr_plot_mkr_0_random_search_002.png
    :alt: Best alphas selected by cross-validation
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 162-165

Compute the predictions on the test set
---------------------------------------
(requires the dual weights)

.. GENERATED FROM PYTHON SOURCE LINES 165-177

.. code-block:: default


    split = False
    scores = predict_and_score_weighted_kernel_ridge(
        Ks_test, dual_weights, deltas, Y_test, split=split,
        n_targets_batch=n_targets_batch, score_func=r2_score_split)
    scores = backend.to_numpy(scores)

    plt.hist(scores, np.linspace(0, 1, 50))
    plt.xlabel(r"$R^2$ generalization score")
    plt.title("Histogram over targets")
    plt.show()




.. image:: /_auto_examples/multiple_kernel_ridge/images/sphx_glr_plot_mkr_0_random_search_003.png
    :alt: Histogram over targets
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 178-185

Compute the split predictions on the test set
---------------------------------------------
(requires the dual weights)

Here we apply the dual weights on each kernel separately
(``exp(deltas[i]) * kernel[i]``), and we compute the R\ :sup:`2` scores
(corrected for correlations) of each prediction.

.. GENERATED FROM PYTHON SOURCE LINES 185-198

.. code-block:: default


    split = True
    scores_split = predict_and_score_weighted_kernel_ridge(
        Ks_test, dual_weights, deltas, Y_test, split=split,
        n_targets_batch=n_targets_batch, score_func=r2_score_split)
    scores_split = backend.to_numpy(scores_split)

    for kk, score in enumerate(scores_split):
        plt.hist(score, np.linspace(0, np.max(scores_split), 50), alpha=0.7,
                 label="kernel %d" % kk)
    plt.title(r"Histogram of $R^2$ generalization score split between kernels")
    plt.legend()
    plt.show()



.. image:: /_auto_examples/multiple_kernel_ridge/images/sphx_glr_plot_mkr_0_random_search_004.png
    :alt: Histogram of $R^2$ generalization score split between kernels
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  29.660 seconds)


.. _sphx_glr_download__auto_examples_multiple_kernel_ridge_plot_mkr_0_random_search.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_mkr_0_random_search.py <plot_mkr_0_random_search.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_mkr_0_random_search.ipynb <plot_mkr_0_random_search.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
