{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multiple-kernel ridge solvers\nThis example demonstrates the different strategies to solve the multiple kernel\nridge regression: the *random search*, and the *hyper-gradient descent*.\n\nThe *random-search* strategy samples some kernel weights vectors from a\nDirichlet distribution, then for each vector, it fits a ``KernelRidgeCV`` model\nand computes a cross-validation score for all targets. Then it selects for each\ntarget the kernel weight vector leading to the highest cross-validation score\n(e.g. the highest `R^2` value). Extensively sampling the kernel weights space\nis exponentially expensive with the number of kernels, therefore this method is\ncomputationally expensive for a large number of kernels. However, since it\nreuses most of the computations for all targets, it scales very well with the\nnumber of targets.\n\nThe *hyper-gradient descent* strategy takes a different route. It starts with\nan initial kernel weights vector per target, and updates it iteratively\nfollowing the hyperparameter gradient, computed over cross-validation. As it\ncomputes a hyper-gradient descent for each target, it is more expensive\ncomputationally for large number of targets. However, the hyper-gradient\ndescent scales very well with the number of kernels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom himalaya.backend import set_backend\nfrom himalaya.kernel_ridge import generate_dirichlet_samples\n\nfrom himalaya.kernel_ridge import KernelRidgeCV\nfrom himalaya.kernel_ridge import MultipleKernelRidgeCV\nfrom himalaya.kernel_ridge import Kernelizer\nfrom himalaya.kernel_ridge import ColumnKernelizer\nfrom himalaya.utils import generate_multikernel_dataset\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import set_config\nset_config(display='diagram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we use the ``torch_cuda`` backend, and fit the model on GPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "backend = set_backend(\"torch_cuda\", on_error=\"warn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a random dataset\nWe start by generating some arbitrary scalings per kernel and targets, using\nsamples on a Dirichlet distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_kernels = 3\nn_targets = 50\nn_clusters = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To create some clusters of weights, we take a few kernel weights samples..\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernel_weights = generate_dirichlet_samples(n_clusters, n_kernels,\n                                            concentration=[.3],\n                                            random_state=105)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. then, we duplicate them, and add some noise, to get clusters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "noise = 0.05\nkernel_weights = backend.to_numpy(kernel_weights)\nkernel_weights = np.tile(kernel_weights, (n_targets // n_clusters, 1))\nkernel_weights += np.random.randn(n_targets, n_kernels) * noise\n\n# We finish with a projection on the simplex, making kernel weights sum to one.\nkernel_weights[kernel_weights < 0] = 0.\nkernel_weights /= np.sum(kernel_weights, 1)[:, None]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we generate a random dataset, using the arbitrary scalings.\n\n- X_train : array of shape (n_samples_train, n_features)\n- X_test : array of shape (n_samples_test, n_features)\n- Y_train : array of shape (n_samples_train, n_targets)\n- Y_test : array of shape (n_samples_test, n_targets)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(X_train, X_test, Y_train, Y_test,\n kernel_weights, n_features_list) = generate_multikernel_dataset(\n     n_kernels=n_kernels, n_targets=n_targets, n_samples_train=600,\n     n_samples_test=300, kernel_weights=kernel_weights, random_state=42)\n\nfeature_names = [f\"Feature space {ii}\" for ii in range(len(n_features_list))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a ``ColumnKernelizer``\nWe define a column kernelizer, which we will use to precompute the kernels in\na pipeline.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Find the start and end of each feature space X in Xs\nstart_and_end = np.concatenate([[0], np.cumsum(n_features_list)])\nslices = [\n    slice(start, end)\n    for start, end in zip(start_and_end[:-1], start_and_end[1:])\n]\n\nkernelizers = [(name, Kernelizer(), slice_)\n               for name, slice_ in zip(feature_names, slices)]\ncolumn_kernelizer = ColumnKernelizer(kernelizers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the models\nWe define the first model, using the random search solver.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# (We pregenerate the Dirichlet random samples, to latter plot them.)\nkernel_weights_sampled = generate_dirichlet_samples(n_samples=20,\n                                                    n_kernels=n_kernels,\n                                                    concentration=[1.],\n                                                    random_state=0)\n\nalphas = np.logspace(-10, 10, 41)\nsolver_params = dict(n_iter=kernel_weights_sampled, alphas=alphas,\n                     n_targets_batch=200, n_alphas_batch=20,\n                     n_targets_batch_refit=200, jitter_alphas=True)\n\nmodel_1 = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=\"random_search\",\n                                solver_params=solver_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define the second model, using the hyper_gradient solver.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "solver_params = dict(max_iter=30, n_targets_batch=200, tol=1e-3,\n                     initial_deltas=\"ridgecv\", max_iter_inner_hyper=1,\n                     hyper_gradient_method=\"direct\")\n\nmodel_2 = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=\"hyper_gradient\",\n                                solver_params=solver_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We fit the two models on the train data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe_1 = make_pipeline(column_kernelizer, model_1)\npipe_1.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe_2 = make_pipeline(column_kernelizer, model_2)\npipe_2.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the convergence curves\nFirst convergence curve.\n\nFor the random search, ``cv_scores`` gives the scores for each sampled kernel\nweights vector. The convergence curve is thus the current maximum for each\ntarget.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cv_scores = backend.to_numpy(pipe_1[1].cv_scores_)\ncurrent_max = np.maximum.accumulate(cv_scores, axis=0)\nmean_current_max = np.mean(current_max, axis=1)\n\nx_array = np.arange(1, len(mean_current_max) + 1)\nplt.plot(x_array, mean_current_max, '-o')\nplt.grid(\"on\")\nplt.xlabel(\"Number of kernel weights sampled\")\nplt.ylabel(\"L2 negative loss (higher is better)\")\nplt.title(\"Convergence curve, averaged over targets\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the second convergence curve.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cv_scores = backend.to_numpy(pipe_2[1].cv_scores_)\nmean_cv_scores = np.mean(cv_scores, axis=1)\n\nx_array = np.arange(1, len(mean_cv_scores) + 1)\nplt.plot(x_array, mean_cv_scores, '-o')\nplt.grid(\"on\")\nplt.xlabel(\"Number of gradient iterations\")\nplt.ylabel(\"L2 negative loss (higher is better)\")\nplt.title(\"Convergence curve, averaged over targets\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare with a ``KernelRidgeCV``\nCompare to a baseline ``KernelRidgeCV`` model with all the concatenated\nfeatures. Comparison is performed using the prediction scores on the test\nset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Fit the baseline model ``KernelRidgeCV``\nbaseline = KernelRidgeCV(kernel=\"linear\", alphas=alphas)\nbaseline.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute scores of all models\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores_1 = pipe_1.score(X_test, Y_test)\nscores_1 = backend.to_numpy(scores_1)\n\nscores_2 = pipe_2.score(X_test, Y_test)\nscores_2 = backend.to_numpy(scores_2)\n\nscores_baseline = baseline.score(X_test, Y_test)\nscores_baseline = backend.to_numpy(scores_baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot histograms\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bins = np.linspace(0, 1, 50)\nplt.hist(scores_baseline, bins, alpha=0.7, label=\"KernelRidgeCV\")\nplt.hist(scores_1, bins, alpha=0.7,\n         label=\"MultipleKernelRidgeCV(solver='random_search')\")\nplt.hist(scores_2, bins, alpha=0.7,\n         label=\"MultipleKernelRidgeCV(solver='hyper_gradient')\")\nplt.xlabel(r\"$R^2$ generalization score\")\nplt.title(\"Histogram over targets\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate trajectories\nRefit the second model with different number of iterations, just to plot the\ntrajectories.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "all_kernel_weights_2 = [\n    np.full((n_targets, n_kernels), fill_value=1. / n_kernels),\n]\nmax_iter = model_2.solver_params[\"max_iter\"]\nfor n_iter in np.unique(np.int_(np.logspace(0, np.log10(max_iter), 3))):\n    # change the number of iteration and refit from scratch\n    pipe_2[1].solver_params['max_iter'] = n_iter\n    pipe_2.fit(X_train, Y_train)\n\n    kernel_weights_2 = np.exp(backend.to_numpy(pipe_2[1].deltas_.T))\n    kernel_weights_2 /= kernel_weights_2.sum(1)[:, None]\n    all_kernel_weights_2.append(kernel_weights_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the normalized kernel weights for the first model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernel_weights_1 = np.exp(backend.to_numpy(pipe_1[1].deltas_.T))\nkernel_weights_1 /= kernel_weights_1.sum(1)[:, None]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot on the simplex\nFinally, we visualize the obtained kernel weights vector, projected on the\nsimplex. The simplex is the space of positive weights that sum to one, and it\nhas a triangular shape in dimension 3.\n\nWe plot on three different panels:\n\n- the kernel weights used in the simulated data\n- the kernel weights sampled during random search, and the best ones\n- the kernel weights trajectories obtained during hyper-gradient descent\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _create_simplex_projection_and_edges(ax):\n    \"\"\"Create a projection on the 3D simplex, and plot edges.\"\"\"\n    n_kernels = 3\n\n    if ax is None:\n        ax = plt.gca()\n\n    # create a projection in 2D\n    from sklearn.decomposition import PCA\n    kernel_weights = generate_dirichlet_samples(10000, n_kernels,\n                                                concentration=[1.],\n                                                random_state=0)\n    pca = PCA(2).fit(backend.to_numpy(kernel_weights))\n\n    # add simplex edges\n    edges = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]])\n    edges = pca.transform(edges).T\n\n    # add tripod at origin\n    tripod_length = 0.15\n    tripod = np.array([[0, 0, 0], [tripod_length, 0, 0], [0, 0, 0],\n                       [0, tripod_length, 0], [0, 0, 0], [0, 0,\n                                                          tripod_length]])\n    tripod = pca.transform(tripod).T\n\n    # add point legend\n    points = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    labels = points.copy()\n    points = pca.transform(points * 1.15).T\n    for (xx, yy), label in zip(points.T, labels):\n        ax.text(xx, yy, str(label), horizontalalignment='center',\n                verticalalignment='center')\n\n    if ax is None:\n        plt.figure(figsize=(8, 8))\n        ax = plt.gca()\n    ax.plot(edges[0], edges[1], c='gray')\n    ax.plot(tripod[0], tripod[1], c='gray')\n    ax.axis('equal')\n    ax.axis('off')\n    return ax, pca\n\n\ndef plot_simplex(X, ax=None, **kwargs):\n    \"\"\"Plot a set of points in the 3D simplex.\"\"\"\n    ax, pca = _create_simplex_projection_and_edges(ax=ax)\n\n    Xt = pca.transform(X).T\n    ax.scatter(Xt[0], Xt[1], **kwargs)\n    ax.legend()\n    return ax\n\n\ndef plot_simplex_trajectory(Xs, ax=None):\n    \"\"\"Plot a series of trajectory in the 3D simplex.\"\"\"\n    ax, pca = _create_simplex_projection_and_edges(ax=ax)\n\n    trajectories = []\n    for Xi in Xs:\n        Xt = pca.transform(Xi).T\n        trajectories.append(Xt)\n    trajectories = np.array(trajectories)\n\n    for trajectory in trajectories.T:\n        ax.plot(trajectory[0], trajectory[1], linewidth=1, color=\"C0\",\n                zorder=1)\n        ax.scatter(trajectory[0, -1], trajectory[1, -1], color=\"C1\", zorder=2)\n\n    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n\n# selection of targets\nselection = slice(0, 50)\n\n# First panel\nax = axs[0]\nax.set_title(\"(a) Ground truth\", y=0)\nplot_simplex(kernel_weights[selection], ax=ax, color='C2',\n             label=\"true weights\")\n\n# Second panel\nax = axs[1]\nax.set_title(\"(b) Random search\", y=0)\nplot_simplex(backend.to_numpy(kernel_weights_sampled), ax=ax, marker='+',\n             label=\"random candidates\", zorder=10)\nplot_simplex(kernel_weights_1[selection], ax=axs[1],\n             label=\"selected candidates\")\n\n# Third panel\nax = axs[2]\nax.set_title(\"(c) Gradient descent\", y=0)\nplot_simplex_trajectory([aa[selection] for aa in all_kernel_weights_2], ax=ax)\nax.legend([ax.lines[2], ax.collections[0]],\n          ['gradient trajectory', 'final point'])\n\nplt.tight_layout()\n# fig.savefig('simulation.pdf', dpi=150, bbox_inches='tight', pad_inches=0)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}