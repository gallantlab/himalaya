{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multiple-kernel ridge refining\nThis example demonstrates how to solve multiple-kernel ridge regression with\nhyperparameter random search, then refine the results with hyperparameter\ngradient descent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom himalaya.backend import set_backend\nfrom himalaya.kernel_ridge import MultipleKernelRidgeCV\nfrom himalaya.kernel_ridge import Kernelizer\nfrom himalaya.kernel_ridge import ColumnKernelizer\nfrom himalaya.utils import generate_multikernel_dataset\n\nfrom sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we use the ``cupy`` backend (GPU).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "backend = set_backend(\"cupy\", on_error=\"warn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can display the ``scikit-learn`` pipeline with an HTML diagram.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn import set_config\nset_config(display='diagram')  # requires scikit-learn 0.23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a random dataset\n- X_train : array of shape (n_samples_train, n_features)\n- X_test : array of shape (n_samples_test, n_features)\n- Y_train : array of shape (n_samples_train, n_targets)\n- Y_test : array of shape (n_samples_test, n_targets)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(X_train, X_test, Y_train, Y_test, kernel_weights,\n n_features_list) = generate_multikernel_dataset(n_kernels=4, n_targets=50,\n                                                 n_samples_train=600,\n                                                 n_samples_test=300,\n                                                 random_state=42)\n\nfeature_names = [f\"Feature space {ii}\" for ii in range(len(n_features_list))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the pipeline\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Find the start and end of each feature space X in Xs\nstart_and_end = np.concatenate([[0], np.cumsum(n_features_list)])\nslices = [\n    slice(start, end)\n    for start, end in zip(start_and_end[:-1], start_and_end[1:])\n]\n\n# Create a different ``Kernelizer`` for each feature space.\nkernelizers = [(\"space %d\" % ii, Kernelizer(), slice_)\n               for ii, slice_ in enumerate(slices)]\ncolumn_kernelizer = ColumnKernelizer(kernelizers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the random-search model\nWe use very few iteration on purpose, to make the random search suboptimal,\nand refine it with hyperparameter gradient descent.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "solver_params = dict(n_iter=5, alphas=np.logspace(-10, 10, 41))\n\nmodel_1 = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=\"random_search\",\n                                solver_params=solver_params, random_state=42)\npipe_1 = make_pipeline(column_kernelizer, model_1)\n\n# Fit the model on all targets\npipe_1.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the gradient-descent model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "solver_params = dict(max_iter=10, hyper_gradient_method=\"direct\",\n                     max_iter_inner_hyper=10,\n                     initial_deltas=\"here_will_go_the_previous_deltas\")\n\nmodel_2 = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=\"hyper_gradient\",\n                                solver_params=solver_params)\npipe_2 = make_pipeline(column_kernelizer, model_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use the random-search to initialize the gradient-descent\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We might want to refine only the best predicting targets, since the\n# hyperparameter gradient descent is less efficient over many targets.\ntop = 60  # top 60%\nbest_cv_scores = backend.to_numpy(pipe_1[-1].cv_scores_.max(0))\nmask = best_cv_scores > np.percentile(best_cv_scores, 100 - top)\n\npipe_2[-1].solver_params['initial_deltas'] = pipe_1[-1].deltas_[:, mask]\npipe_2.fit(X_train, Y_train[:, mask])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute predictions on a test set\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\n# use the first model for all targets\ntest_scores_1 = pipe_1.score(X_test, Y_test)\n\n# use the second model for the refined targets\ntest_scores_2 = backend.copy(test_scores_1)\ntest_scores_2[mask] = pipe_2.score(X_test, Y_test[:, mask])\n\ntest_scores_1 = backend.to_numpy(test_scores_1)\ntest_scores_2 = backend.to_numpy(test_scores_2)\nplt.figure(figsize=(4, 4))\nplt.scatter(test_scores_1, test_scores_2, alpha=0.3)\nplt.xlim(0, 1)\nplt.plot(plt.xlim(), plt.xlim(), color='k', lw=1)\nplt.xlabel(r\"Base model\")\nplt.ylabel(r\"Refined model\")\nplt.title(\"$R^2$ generalization score\")\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}