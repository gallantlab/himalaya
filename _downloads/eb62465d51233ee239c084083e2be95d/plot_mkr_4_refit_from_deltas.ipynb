{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multiple-kernel ridge fit from fixed hyper-parameters\nThis example demonstrates how to fit a multiple-kernel ridge model with fixed\nhyper-parameters. Here are three different usecases:\n\n- If the kernel weights hyper-parameters are known and identical across\n  targets, the kernels can be scaled and summed, and a simple KernelRidgeCV can\n  be used to fit the model.\n- If the kernel weights hyper-parameters are unknown and different across\n  targets, a MultipleKernelRidgeCV can be use to search the best\n  hyper-parameters per target.\n- If the kernel weights hyper-parameters are known and different across\n  targets, a WeightedKernelRidge model can be used to fit the ridge models on\n  each target independently.\n\nThis method can be used for example in the following workflow:\n\n- fit a MultipleKernelRidgeCV to learn the kernel weights hyper-parameter,\n- save the hyper-parameters, but not the ridge weights to save disk space,\n- fit a WeightedKernelRidge from the saved hyper-parameters, for further use of\n  the model (prediction, interpretation, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom himalaya.backend import set_backend\nfrom himalaya.kernel_ridge import WeightedKernelRidge\nfrom himalaya.kernel_ridge import Kernelizer\nfrom himalaya.kernel_ridge import ColumnKernelizer\nfrom himalaya.utils import generate_multikernel_dataset\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import set_config\nset_config(display='diagram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we use the ``torch_cuda`` backend (GPU).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "backend = set_backend(\"torch_cuda\", on_error=\"warn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a random dataset\n- X_train : array of shape (n_samples_train, n_features)\n- X_test : array of shape (n_samples_test, n_features)\n- Y_train : array of shape (n_samples_train, n_targets)\n- Y_test : array of shape (n_samples_test, n_targets)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(X_train, X_test, Y_train, Y_test, kernel_weights,\n n_features_list) = generate_multikernel_dataset(n_kernels=4, n_targets=500,\n                                                 n_samples_train=1000,\n                                                 n_samples_test=400,\n                                                 random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the pipeline\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Find the start and end of each feature space X in Xs\nstart_and_end = np.concatenate([[0], np.cumsum(n_features_list)])\nslices = [\n    slice(start, end)\n    for start, end in zip(start_and_end[:-1], start_and_end[1:])\n]\n\n# Create a different ``Kernelizer`` for each feature space.\nkernelizers = [(\"space %d\" % ii, Kernelizer(), slice_)\n               for ii, slice_ in enumerate(slices)]\ncolumn_kernelizer = ColumnKernelizer(kernelizers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the weighted kernel ridge model\nHere we use the ground truth kernel weights for each target (deltas), but it\ncan be typically used with deltas obtained from a MultipleKernelRidgeCV fit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "deltas = backend.log(backend.asarray(kernel_weights.T))\n\nmodel_1 = WeightedKernelRidge(alpha=1, deltas=deltas, kernels=\"precomputed\")\npipe_1 = make_pipeline(column_kernelizer, model_1)\n\n# Fit the model on all targets\npipe_1.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "compute test score\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_scores_1 = pipe_1.score(X_test, Y_test)\ntest_scores_1 = backend.to_numpy(test_scores_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compare this model to a baseline model where the kernel weights are\nall equal and not learnt.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_2 = WeightedKernelRidge(alpha=1, deltas=\"zeros\", kernels=\"precomputed\")\npipe_2 = make_pipeline(column_kernelizer, model_2)\n\n# Fit the model on all targets\npipe_2.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "compute test score\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_scores_2 = pipe_2.score(X_test, Y_test)\ntest_scores_2 = backend.to_numpy(test_scores_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare the predictions on a test set\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(4, 3))\nplt.hist(test_scores_2, np.linspace(0, 1, 30), alpha=0.7,\n         label=\"Default deltas\")\nplt.hist(test_scores_1, np.linspace(0, 1, 30), alpha=0.7,\n         label=\"Ground truth deltas\")\nplt.xlabel(\"$R^2$ generalization score\")\nplt.ylabel(\"Number of voxels\")\nplt.legend()\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}