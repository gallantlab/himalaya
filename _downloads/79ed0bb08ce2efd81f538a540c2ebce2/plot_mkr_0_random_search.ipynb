{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multiple-kernel ridge\nThis example demonstrates how to solve multiple kernel ridge regression.\nIt uses random search and cross validation to select optimal hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom himalaya.backend import set_backend\nfrom himalaya.kernel_ridge import solve_multiple_kernel_ridge_random_search\nfrom himalaya.kernel_ridge import predict_and_score_weighted_kernel_ridge\nfrom himalaya.utils import generate_multikernel_dataset\nfrom himalaya.scoring import r2_score_split\nfrom himalaya.viz import plot_alphas_diagnostic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we use the ``cupy`` backend, and fit the model on GPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "backend = set_backend(\"cupy\", on_error=\"warn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a random dataset\n\n- X_train : array of shape (n_samples_train, n_features)\n- X_test : array of shape (n_samples_test, n_features)\n- Y_train : array of shape (n_samples_train, n_targets)\n- Y_test : array of shape (n_samples_test, n_targets)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_kernels = 3\nn_targets = 50\nkernel_weights = np.tile(np.array([0.5, 0.3, 0.2])[None], (n_targets, 1))\n\n(X_train, X_test, Y_train, Y_test,\n kernel_weights, n_features_list) = generate_multikernel_dataset(\n     n_kernels=n_kernels, n_targets=n_targets, n_samples_train=600,\n     n_samples_test=300, kernel_weights=kernel_weights, random_state=42)\n\nfeature_names = [f\"Feature space {ii}\" for ii in range(len(n_features_list))]\n\n# Find the start and end of each feature space X in Xs\nstart_and_end = np.concatenate([[0], np.cumsum(n_features_list)])\nslices = [\n    slice(start, end)\n    for start, end in zip(start_and_end[:-1], start_and_end[1:])\n]\nXs_train = [X_train[:, slic] for slic in slices]\nXs_test = [X_test[:, slic] for slic in slices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precompute the linear kernels\nWe also cast them to float32.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Ks_train = backend.stack([X_train @ X_train.T for X_train in Xs_train])\nKs_train = backend.asarray(Ks_train, dtype=backend.float32)\nY_train = backend.asarray(Y_train, dtype=backend.float32)\n\nKs_test = backend.stack(\n    [X_test @ X_train.T for X_train, X_test in zip(Xs_train, Xs_test)])\nKs_test = backend.asarray(Ks_test, dtype=backend.float32)\nY_test = backend.asarray(Y_test, dtype=backend.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the solver, using random search\nThis method should work fine for\nsmall number of kernels (< 20). The larger the number of kenels, the larger\nwe need to sample the hyperparameter space (i.e. increasing ``n_iter``).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we use 100 iterations to have a reasonably fast example (~40 sec).\nTo have a better convergence, we probably need more iterations.\nNote that there is currently no stopping criterion in this method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_iter = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Grid of regularization parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alphas = np.logspace(-10, 10, 21)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Batch parameters are used to reduce the necessary GPU memory. A larger value\nwill be a bit faster, but the solver might crash if it runs out of memory.\nOptimal values depend on the size of your dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_targets_batch = 1000\nn_alphas_batch = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If ``return_weights == \"dual\"``, the solver will use more memory.\nTo mitigate this, you can reduce ``n_targets_batch`` in the refit\nusing ```n_targets_batch_refit``.\nIf you don't need the dual weights, use ``return_weights = None``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "return_weights = 'dual'\nn_targets_batch_refit = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the solver. For each iteration, it will:\n\n- sample kernel weights from a Dirichlet distribution\n- fit (n_splits * n_alphas * n_targets) ridge models\n- compute the scores on the validation set of each split\n- average the scores over splits\n- take the maximum over alphas\n- (only if you ask for the ridge weights) refit using the best alphas per\n  target and the entire dataset\n- return for each target the log kernel weights leading to the best CV score\n  (and the best weights if necessary)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = solve_multiple_kernel_ridge_random_search(\n    Ks=Ks_train,\n    Y=Y_train,\n    n_iter=n_iter,\n    alphas=alphas,\n    n_targets_batch=n_targets_batch,\n    return_weights=return_weights,\n    n_alphas_batch=n_alphas_batch,\n    n_targets_batch_refit=n_targets_batch_refit,\n    jitter_alphas=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we used the ``cupy`` backend, the results are ``cupy`` arrays, which are\non GPU. Here, we cast the results back to CPU, and to ``numpy`` arrays.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "deltas = backend.to_numpy(results[0])\ndual_weights = backend.to_numpy(results[1])\ncv_scores = backend.to_numpy(results[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the convergence curve\n\n``cv_scores`` gives the scores for each sampled kernel weights.\nThe convergence curve is thus the current maximum for each target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "current_max = np.maximum.accumulate(cv_scores, axis=0)\nmean_current_max = np.mean(current_max, axis=1)\nx_array = np.arange(1, len(mean_current_max) + 1)\nplt.plot(x_array, mean_current_max, '-o')\nplt.grid(\"on\")\nplt.xlabel(\"Number of kernel weights sampled\")\nplt.ylabel(\"L2 negative loss (higher is better)\")\nplt.title(\"Convergence curve, averaged over targets\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the optimal alphas selected by the solver\n\nThis plot is helpful to refine the alpha grid if the range is too small or\ntoo large.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "best_alphas = 1. / np.sum(np.exp(deltas), axis=0)\nplot_alphas_diagnostic(best_alphas, alphas)\nplt.title(\"Best alphas selected by cross-validation\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the predictions on the test set\n(requires the dual weights)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "split = False\nscores = predict_and_score_weighted_kernel_ridge(\n    Ks_test, dual_weights, deltas, Y_test, split=split,\n    n_targets_batch=n_targets_batch, score_func=r2_score_split)\nscores = backend.to_numpy(scores)\n\nplt.hist(scores, np.linspace(0, 1, 50))\nplt.xlabel(r\"$R^2$ generalization score\")\nplt.title(\"Histogram over targets\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the split predictions on the test set\n(requires the dual weights)\n\nHere we apply the dual weights on each kernel separately\n(``exp(deltas[i]) * kernel[i]``), and we compute the R\\ :sup:`2` scores\n(corrected for correlations) of each prediction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "split = True\nscores_split = predict_and_score_weighted_kernel_ridge(\n    Ks_test, dual_weights, deltas, Y_test, split=split,\n    n_targets_batch=n_targets_batch, score_func=r2_score_split)\nscores_split = backend.to_numpy(scores_split)\n\nfor kk, score in enumerate(scores_split):\n    plt.hist(score, np.linspace(0, np.max(scores_split), 50), alpha=0.7,\n             label=\"kernel %d\" % kk)\nplt.title(r\"Histogram of $R^2$ generalization score split between kernels\")\nplt.legend()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}