<!DOCTYPE html>

<html lang="en" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Multiple-kernel ridge &#8212; Himalaya 0.4.10 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css?v=e9c93658" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <script src="../../_static/documentation_options.js?v=e3241242"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Multiple-kernel ridge with scikit-learn API" href="plot_mkr_1_sklearn_api.html" />
    <link rel="prev" title="Multiple-kernel ridge" href="index.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  <div class="document">
    
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="../../index.html">
    <img class="logo" src="../../_static/logo.svg" alt="Logo" />
    
    <h1 class="logo logo-name">himalaya</h1>
    
  </a>
</p>






<p>
<iframe src="https://ghbtns.com/github-btn.html?user=gallantlab&repo=himalaya&type=star&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Gallery of examples</a><ul>
  <li><a href="index.html">Multiple-kernel ridge</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">Multiple-kernel ridge</a></li>
      <li>Next: <a href="plot_mkr_1_sklearn_api.html" title="next chapter">Multiple-kernel ridge with scikit-learn API</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div><h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../models.html">Model descriptions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../flowchart.html">Model flowchart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Gallery of examples</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changelog.html">Changelog</a></li>
</ul>


<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-examples-multiple-kernel-ridge-plot-mkr-0-random-search-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="multiple-kernel-ridge">
<span id="sphx-glr-auto-examples-multiple-kernel-ridge-plot-mkr-0-random-search-py"></span><h1>Multiple-kernel ridge<a class="headerlink" href="#multiple-kernel-ridge" title="Link to this heading">¶</a></h1>
<p>This example demonstrates how to solve multiple kernel ridge regression.
It uses random search and cross validation to select optimal hyperparameters.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">himalaya.backend</span><span class="w"> </span><span class="kn">import</span> <span class="n">set_backend</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">himalaya.kernel_ridge</span><span class="w"> </span><span class="kn">import</span> <span class="n">solve_multiple_kernel_ridge_random_search</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">himalaya.kernel_ridge</span><span class="w"> </span><span class="kn">import</span> <span class="n">predict_and_score_weighted_kernel_ridge</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">himalaya.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">generate_multikernel_dataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">himalaya.scoring</span><span class="w"> </span><span class="kn">import</span> <span class="n">r2_score_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">himalaya.viz</span><span class="w"> </span><span class="kn">import</span> <span class="n">plot_alphas_diagnostic</span>
</pre></div>
</div>
<p>In this example, we use the <code class="docutils literal notranslate"><span class="pre">cupy</span></code> backend, and fit the model on GPU.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">backend</span> <span class="o">=</span> <span class="n">set_backend</span><span class="p">(</span><span class="s2">&quot;cupy&quot;</span><span class="p">,</span> <span class="n">on_error</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/home/runner/work/himalaya/himalaya/himalaya/backend/_utils.py:55: UserWarning: Setting backend to cupy failed: Cupy not installed..Falling back to numpy backend.
  warnings.warn(f&quot;Setting backend to {backend} failed: {str(error)}.&quot;
</pre></div>
</div>
<section id="generate-a-random-dataset">
<h2>Generate a random dataset<a class="headerlink" href="#generate-a-random-dataset" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>X_train : array of shape (n_samples_train, n_features)</p></li>
<li><p>X_test : array of shape (n_samples_test, n_features)</p></li>
<li><p>Y_train : array of shape (n_samples_train, n_targets)</p></li>
<li><p>Y_test : array of shape (n_samples_test, n_targets)</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">n_kernels</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_targets</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">kernel_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])[</span><span class="kc">None</span><span class="p">],</span> <span class="p">(</span><span class="n">n_targets</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span>
 <span class="n">kernel_weights</span><span class="p">,</span> <span class="n">n_features_list</span><span class="p">)</span> <span class="o">=</span> <span class="n">generate_multikernel_dataset</span><span class="p">(</span>
     <span class="n">n_kernels</span><span class="o">=</span><span class="n">n_kernels</span><span class="p">,</span> <span class="n">n_targets</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">n_samples_train</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span>
     <span class="n">n_samples_test</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">kernel_weights</span><span class="o">=</span><span class="n">kernel_weights</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Feature space </span><span class="si">{</span><span class="n">ii</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">n_features_list</span><span class="p">))]</span>

<span class="c1"># Find the start and end of each feature space X in Xs</span>
<span class="n">start_and_end</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">n_features_list</span><span class="p">)])</span>
<span class="n">slices</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nb">slice</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">start_and_end</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">start_and_end</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="p">]</span>
<span class="n">Xs_train</span> <span class="o">=</span> <span class="p">[</span><span class="n">X_train</span><span class="p">[:,</span> <span class="n">slic</span><span class="p">]</span> <span class="k">for</span> <span class="n">slic</span> <span class="ow">in</span> <span class="n">slices</span><span class="p">]</span>
<span class="n">Xs_test</span> <span class="o">=</span> <span class="p">[</span><span class="n">X_test</span><span class="p">[:,</span> <span class="n">slic</span><span class="p">]</span> <span class="k">for</span> <span class="n">slic</span> <span class="ow">in</span> <span class="n">slices</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="precompute-the-linear-kernels">
<h2>Precompute the linear kernels<a class="headerlink" href="#precompute-the-linear-kernels" title="Link to this heading">¶</a></h2>
<p>We also cast them to float32.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">Ks_train</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">X_train</span> <span class="o">@</span> <span class="n">X_train</span><span class="o">.</span><span class="n">T</span> <span class="k">for</span> <span class="n">X_train</span> <span class="ow">in</span> <span class="n">Xs_train</span><span class="p">])</span>
<span class="n">Ks_train</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">Ks_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">backend</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">backend</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">Ks_test</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
    <span class="p">[</span><span class="n">X_test</span> <span class="o">@</span> <span class="n">X_train</span><span class="o">.</span><span class="n">T</span> <span class="k">for</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">Xs_train</span><span class="p">,</span> <span class="n">Xs_test</span><span class="p">)])</span>
<span class="n">Ks_test</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">Ks_test</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">backend</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">backend</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="run-the-solver-using-random-search">
<h2>Run the solver, using random search<a class="headerlink" href="#run-the-solver-using-random-search" title="Link to this heading">¶</a></h2>
<p>This method should work fine for
small number of kernels (&lt; 20). The larger the number of kernels, the larger
we need to sample the hyperparameter space (i.e. increasing <code class="docutils literal notranslate"><span class="pre">n_iter</span></code>).</p>
<p>Here we use 100 iterations to have a reasonably fast example (~40 sec).
To have a better convergence, we probably need more iterations.
Note that there is currently no stopping criterion in this method.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">n_iter</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>
</div>
<p>Grid of regularization parameters.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">21</span><span class="p">)</span>
</pre></div>
</div>
<p>Batch parameters are used to reduce the necessary GPU memory. A larger value
will be a bit faster, but the solver might crash if it runs out of memory.
Optimal values depend on the size of your dataset.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">n_targets_batch</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">n_alphas_batch</span> <span class="o">=</span> <span class="mi">20</span>
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">return_weights</span> <span class="pre">==</span> <span class="pre">&quot;dual&quot;</span></code>, the solver will use more memory.
To mitigate this, you can reduce <code class="docutils literal notranslate"><span class="pre">n_targets_batch</span></code> in the refit
using <code class="docutils literal notranslate"><span class="pre">`n_targets_batch_refit</span></code>.
If you don’t need the dual weights, use <code class="docutils literal notranslate"><span class="pre">return_weights</span> <span class="pre">=</span> <span class="pre">None</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">return_weights</span> <span class="o">=</span> <span class="s1">&#39;dual&#39;</span>
<span class="n">n_targets_batch_refit</span> <span class="o">=</span> <span class="mi">200</span>
</pre></div>
</div>
<p>Run the solver. For each iteration, it will:</p>
<ul class="simple">
<li><p>sample kernel weights from a Dirichlet distribution</p></li>
<li><p>fit (n_splits * n_alphas * n_targets) ridge models</p></li>
<li><p>compute the scores on the validation set of each split</p></li>
<li><p>average the scores over splits</p></li>
<li><p>take the maximum over alphas</p></li>
<li><p>(only if you ask for the ridge weights) refit using the best alphas per
target and the entire dataset</p></li>
<li><p>return for each target the log kernel weights leading to the best CV score
(and the best weights if necessary)</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">solve_multiple_kernel_ridge_random_search</span><span class="p">(</span>
    <span class="n">Ks</span><span class="o">=</span><span class="n">Ks_train</span><span class="p">,</span>
    <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">,</span>
    <span class="n">n_iter</span><span class="o">=</span><span class="n">n_iter</span><span class="p">,</span>
    <span class="n">alphas</span><span class="o">=</span><span class="n">alphas</span><span class="p">,</span>
    <span class="n">n_targets_batch</span><span class="o">=</span><span class="n">n_targets_batch</span><span class="p">,</span>
    <span class="n">return_weights</span><span class="o">=</span><span class="n">return_weights</span><span class="p">,</span>
    <span class="n">n_alphas_batch</span><span class="o">=</span><span class="n">n_alphas_batch</span><span class="p">,</span>
    <span class="n">n_targets_batch_refit</span><span class="o">=</span><span class="n">n_targets_batch_refit</span><span class="p">,</span>
    <span class="n">jitter_alphas</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[                              ] 0% | 0.00 sec | 100 random sampling with cv |
[                              ] 1% | 1.54 sec | 100 random sampling with cv | 0.65 it/s, ETA: 00:02:32
[                              ] 2% | 2.75 sec | 100 random sampling with cv | 0.73 it/s, ETA: 00:02:14
[                              ] 3% | 3.71 sec | 100 random sampling with cv | 0.81 it/s, ETA: 00:01:59
[.                             ] 4% | 4.77 sec | 100 random sampling with cv | 0.84 it/s, ETA: 00:01:54
[.                             ] 5% | 5.77 sec | 100 random sampling with cv | 0.87 it/s, ETA: 00:01:49
[.                             ] 6% | 7.08 sec | 100 random sampling with cv | 0.85 it/s, ETA: 00:01:50
[..                            ] 7% | 7.96 sec | 100 random sampling with cv | 0.88 it/s, ETA: 00:01:45
[..                            ] 8% | 8.99 sec | 100 random sampling with cv | 0.89 it/s, ETA: 00:01:43
[..                            ] 9% | 10.06 sec | 100 random sampling with cv | 0.89 it/s, ETA: 00:01:41
[...                           ] 10% | 11.32 sec | 100 random sampling with cv | 0.88 it/s, ETA: 00:01:41
[...                           ] 11% | 12.60 sec | 100 random sampling with cv | 0.87 it/s, ETA: 00:01:41
[...                           ] 12% | 13.97 sec | 100 random sampling with cv | 0.86 it/s, ETA: 00:01:42
[...                           ] 13% | 15.06 sec | 100 random sampling with cv | 0.86 it/s, ETA: 00:01:40
[....                          ] 14% | 16.04 sec | 100 random sampling with cv | 0.87 it/s, ETA: 00:01:38
[....                          ] 15% | 17.04 sec | 100 random sampling with cv | 0.88 it/s, ETA: 00:01:36
[....                          ] 16% | 17.99 sec | 100 random sampling with cv | 0.89 it/s, ETA: 00:01:34
[.....                         ] 17% | 18.75 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:01:31
[.....                         ] 18% | 19.69 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:01:29
[.....                         ] 19% | 20.83 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:01:28
[......                        ] 20% | 21.83 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:01:27
[......                        ] 21% | 22.86 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:01:26
[......                        ] 22% | 23.94 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:01:24
[......                        ] 23% | 24.55 sec | 100 random sampling with cv | 0.94 it/s, ETA: 00:01:22
[.......                       ] 24% | 25.75 sec | 100 random sampling with cv | 0.93 it/s, ETA: 00:01:21
[.......                       ] 25% | 26.89 sec | 100 random sampling with cv | 0.93 it/s, ETA: 00:01:20
[.......                       ] 26% | 28.11 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:01:20
[........                      ] 27% | 29.28 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:01:19
[........                      ] 28% | 30.11 sec | 100 random sampling with cv | 0.93 it/s, ETA: 00:01:17
[........                      ] 29% | 31.18 sec | 100 random sampling with cv | 0.93 it/s, ETA: 00:01:16
[.........                     ] 30% | 32.39 sec | 100 random sampling with cv | 0.93 it/s, ETA: 00:01:15
[.........                     ] 31% | 33.49 sec | 100 random sampling with cv | 0.93 it/s, ETA: 00:01:14
[.........                     ] 32% | 34.56 sec | 100 random sampling with cv | 0.93 it/s, ETA: 00:01:13
[.........                     ] 33% | 35.63 sec | 100 random sampling with cv | 0.93 it/s, ETA: 00:01:12
[..........                    ] 34% | 36.80 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:01:11
[..........                    ] 35% | 37.73 sec | 100 random sampling with cv | 0.93 it/s, ETA: 00:01:10
[..........                    ] 36% | 38.91 sec | 100 random sampling with cv | 0.93 it/s, ETA: 00:01:09
[...........                   ] 37% | 39.90 sec | 100 random sampling with cv | 0.93 it/s, ETA: 00:01:07
[...........                   ] 38% | 41.10 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:01:07
[...........                   ] 39% | 42.35 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:01:06
[............                  ] 40% | 43.39 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:01:05
[............                  ] 41% | 44.51 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:01:04
[............                  ] 42% | 45.24 sec | 100 random sampling with cv | 0.93 it/s, ETA: 00:01:02
[............                  ] 43% | 46.36 sec | 100 random sampling with cv | 0.93 it/s, ETA: 00:01:01
[.............                 ] 44% | 47.82 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:01:00
[.............                 ] 45% | 48.81 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:00:59
[.............                 ] 46% | 49.90 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:00:58
[..............                ] 47% | 51.08 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:00:57
[..............                ] 48% | 52.24 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:00:56
[..............                ] 49% | 53.55 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:55
[...............               ] 50% | 54.40 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:00:54
[...............               ] 51% | 55.47 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:00:53
[...............               ] 52% | 56.46 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:00:52
[...............               ] 53% | 57.78 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:00:51
[................              ] 54% | 58.88 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:00:50
[................              ] 55% | 60.01 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:00:49
[................              ] 56% | 61.14 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:00:48
[.................             ] 57% | 62.37 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:47
[.................             ] 58% | 63.39 sec | 100 random sampling with cv | 0.92 it/s, ETA: 00:00:45
[.................             ] 59% | 64.49 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:44
[..................            ] 60% | 65.77 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:43
[..................            ] 61% | 66.85 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:42
[..................            ] 62% | 68.22 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:41
[..................            ] 63% | 69.42 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:40
[...................           ] 64% | 70.54 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:39
[...................           ] 65% | 71.74 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:38
[...................           ] 66% | 72.63 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:37
[....................          ] 67% | 73.79 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:36
[....................          ] 68% | 75.00 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:35
[....................          ] 69% | 76.23 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:34
[.....................         ] 70% | 77.37 sec | 100 random sampling with cv | 0.90 it/s, ETA: 00:00:33
[.....................         ] 71% | 78.44 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:32
[.....................         ] 72% | 79.55 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:30
[.....................         ] 73% | 80.63 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:29
[......................        ] 74% | 81.81 sec | 100 random sampling with cv | 0.90 it/s, ETA: 00:00:28
[......................        ] 75% | 82.84 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:27
[......................        ] 76% | 84.26 sec | 100 random sampling with cv | 0.90 it/s, ETA: 00:00:26
[.......................       ] 77% | 85.41 sec | 100 random sampling with cv | 0.90 it/s, ETA: 00:00:25
[.......................       ] 78% | 86.59 sec | 100 random sampling with cv | 0.90 it/s, ETA: 00:00:24
[.......................       ] 79% | 87.62 sec | 100 random sampling with cv | 0.90 it/s, ETA: 00:00:23
[........................      ] 80% | 88.69 sec | 100 random sampling with cv | 0.90 it/s, ETA: 00:00:22
[........................      ] 81% | 89.77 sec | 100 random sampling with cv | 0.90 it/s, ETA: 00:00:21
[........................      ] 82% | 90.89 sec | 100 random sampling with cv | 0.90 it/s, ETA: 00:00:19
[........................      ] 83% | 92.19 sec | 100 random sampling with cv | 0.90 it/s, ETA: 00:00:18
[.........................     ] 84% | 93.07 sec | 100 random sampling with cv | 0.90 it/s, ETA: 00:00:17
[.........................     ] 85% | 94.00 sec | 100 random sampling with cv | 0.90 it/s, ETA: 00:00:16
[.........................     ] 86% | 95.19 sec | 100 random sampling with cv | 0.90 it/s, ETA: 00:00:15
[..........................    ] 87% | 96.24 sec | 100 random sampling with cv | 0.90 it/s, ETA: 00:00:14
[..........................    ] 88% | 97.37 sec | 100 random sampling with cv | 0.90 it/s, ETA: 00:00:13
[..........................    ] 89% | 98.32 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:12
[...........................   ] 90% | 99.31 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:11
[...........................   ] 91% | 100.55 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:09
[...........................   ] 92% | 101.62 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:08
[...........................   ] 93% | 102.52 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:07
[............................  ] 94% | 103.41 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:06
[............................  ] 95% | 104.47 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:05
[............................  ] 96% | 105.47 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:04
[............................. ] 97% | 106.64 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:03
[............................. ] 98% | 107.68 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:02
[............................. ] 99% | 108.56 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:01
[..............................] 100% | 109.45 sec | 100 random sampling with cv | 0.91 it/s, ETA: 00:00:00
</pre></div>
</div>
<p>As we used the <code class="docutils literal notranslate"><span class="pre">cupy</span></code> backend, the results are <code class="docutils literal notranslate"><span class="pre">cupy</span></code> arrays, which are
on GPU. Here, we cast the results back to CPU, and to <code class="docutils literal notranslate"><span class="pre">numpy</span></code> arrays.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">deltas</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">dual_weights</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">cv_scores</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="plot-the-convergence-curve">
<h2>Plot the convergence curve<a class="headerlink" href="#plot-the-convergence-curve" title="Link to this heading">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">cv_scores</span></code> gives the scores for each sampled kernel weights.
The convergence curve is thus the current maximum for each target.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">current_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="o">.</span><span class="n">accumulate</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">mean_current_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">current_max</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">mean_current_max</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_array</span><span class="p">,</span> <span class="n">mean_current_max</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of kernel weights sampled&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;L2 negative loss (higher is better)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Convergence curve, averaged over targets&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_mkr_0_random_search_001.png" srcset="../../_images/sphx_glr_plot_mkr_0_random_search_001.png" alt="Convergence curve, averaged over targets" class = "sphx-glr-single-img"/></section>
<section id="plot-the-optimal-alphas-selected-by-the-solver">
<h2>Plot the optimal alphas selected by the solver<a class="headerlink" href="#plot-the-optimal-alphas-selected-by-the-solver" title="Link to this heading">¶</a></h2>
<p>This plot is helpful to refine the alpha grid if the range is too small or
too large.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">best_alphas</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">deltas</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plot_alphas_diagnostic</span><span class="p">(</span><span class="n">best_alphas</span><span class="p">,</span> <span class="n">alphas</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Best alphas selected by cross-validation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_mkr_0_random_search_002.png" srcset="../../_images/sphx_glr_plot_mkr_0_random_search_002.png" alt="Best alphas selected by cross-validation" class = "sphx-glr-single-img"/></section>
<section id="compute-the-predictions-on-the-test-set">
<h2>Compute the predictions on the test set<a class="headerlink" href="#compute-the-predictions-on-the-test-set" title="Link to this heading">¶</a></h2>
<p>(requires the dual weights)</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">split</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">predict_and_score_weighted_kernel_ridge</span><span class="p">(</span>
    <span class="n">Ks_test</span><span class="p">,</span> <span class="n">dual_weights</span><span class="p">,</span> <span class="n">deltas</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="n">split</span><span class="p">,</span>
    <span class="n">n_targets_batch</span><span class="o">=</span><span class="n">n_targets_batch</span><span class="p">,</span> <span class="n">score_func</span><span class="o">=</span><span class="n">r2_score_split</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$R^2$ generalization score&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Histogram over targets&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_mkr_0_random_search_003.png" srcset="../../_images/sphx_glr_plot_mkr_0_random_search_003.png" alt="Histogram over targets" class = "sphx-glr-single-img"/></section>
<section id="compute-the-split-predictions-on-the-test-set">
<h2>Compute the split predictions on the test set<a class="headerlink" href="#compute-the-split-predictions-on-the-test-set" title="Link to this heading">¶</a></h2>
<p>(requires the dual weights)</p>
<p>Here we apply the dual weights on each kernel separately
(<code class="docutils literal notranslate"><span class="pre">exp(deltas[i])</span> <span class="pre">*</span> <span class="pre">kernel[i]</span></code>), and we compute the R<sup>2</sup> scores
(corrected for correlations) of each prediction.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">split</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">scores_split</span> <span class="o">=</span> <span class="n">predict_and_score_weighted_kernel_ridge</span><span class="p">(</span>
    <span class="n">Ks_test</span><span class="p">,</span> <span class="n">dual_weights</span><span class="p">,</span> <span class="n">deltas</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="n">split</span><span class="p">,</span>
    <span class="n">n_targets_batch</span><span class="o">=</span><span class="n">n_targets_batch</span><span class="p">,</span> <span class="n">score_func</span><span class="o">=</span><span class="n">r2_score_split</span><span class="p">)</span>
<span class="n">scores_split</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(</span><span class="n">scores_split</span><span class="p">)</span>

<span class="k">for</span> <span class="n">kk</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">scores_split</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">scores_split</span><span class="p">),</span> <span class="mi">50</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="s2">&quot;kernel </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">kk</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Histogram of $R^2$ generalization score split between kernels&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_mkr_0_random_search_004.png" srcset="../../_images/sphx_glr_plot_mkr_0_random_search_004.png" alt="Histogram of $R^2$ generalization score split between kernels" class = "sphx-glr-single-img"/><p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (1 minutes 50.164 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-multiple-kernel-ridge-plot-mkr-0-random-search-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/79ed0bb08ce2efd81f538a540c2ebce2/plot_mkr_0_random_search.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_mkr_0_random_search.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/653b82cbcba3e05b6b8ac445ef39933b/plot_mkr_0_random_search.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_mkr_0_random_search.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/09beab01ce982ef5aa54fae76740a69c/plot_mkr_0_random_search.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">plot_mkr_0_random_search.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &#169;2023, Gallant lab.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.4.7</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="../../_sources/_auto_examples/multiple_kernel_ridge/plot_mkr_0_random_search.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>